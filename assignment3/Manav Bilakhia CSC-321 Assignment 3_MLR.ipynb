{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.8"},"colab":{"provenance":[{"file_id":"11wcVP2KvGIpCq5jUybNsNDWLh-kh25sW","timestamp":1681142370423}]}},"cells":[{"cell_type":"markdown","metadata":{"id":"urq6CK_yJwQf"},"source":["# CSC-321: Data Mining and Machine Learning\n","\n","# Manav Bilakhia\n","\n","## Assignment 3: Multivariate Linear Regression\n","\n","\n","The function for making predicitions using Simple Linear Regression was:\n","\n","**y = b0 + b1*x**\n","\n","For the more complicated Multivariate Linear Regression the function is:\n","\n","**y = b0 + b1*x1 + b2*x2 + ... + bN*xN**\n","\n","With there being as many coefficients (b) as there are input features, plus 1 (for the b0 coefficient, or intercept). \n","\n","For more complex learning, we cannot simply read our coefficients (b0 and b1 for SLR) from the data. We *could* use the least squares method, but for MORE input coefficients we have to use more complex linear algebra that involves lots of matrix math. And in the end it will be an *estimation* of the coefficients. \n","\n","We have to estimate them because there are many more of them, and they all interact together. Least squares is one method of estimation. We're going to look at another, Stochastic Gradient Descent (SGD).\n","\n","We don't know exactly how the input variables play together for a given data set so we're going to use a mechanism to pick some random values for our initial coefficients, and gradually improve the coefficients over time.\n","\n","The method we discussed in class is called Stochastic Gradient Descent, and is one of a variety of optimization algorithms that are used in machine learning methods to 'learn' the coefficients, or weights, on input values with respect to some output. This estimation of coefficients IS the learning part of machine learning. At least for these linear models.\n","\n","Gradient descent is the process of minimizing some function (we'll call it the cost, or error function) by following the slope (or gradient) of the function down to some minimum (the lowest error over our data). \n","\n","Intuitively, we're going to show our model one training instance at a time, make a prediction for that instance, calculate the error using that instance, and update the model in order to improve performance (get a smaller error) for the next prediction. We'll repeat this process for a number of iterations, minimizing the error each time.\n","\n","Each iteration, we're going to update each of the coefficients using the formula:\n","\n","b = b - learning_rate * error * value_of_X\n","\n","for each coefficient (again, corresponding to each input feature, of each instance). The error means the error calculated when we use the coefficients to make a prediction. So for coefficient b1, this translates to:\n","\n","b1 = b1 - learning_rate * error * value_of_x1\n","\n","Remember also that learning_rate is a value we must choose (I'll tell you to start with), and the total number of iterations over all the training data (epochs) is ALSO a number we must choose. \n","\n","We're going to break this down into the individual steps. I recommend reading this whole notebook to get a sense of where we're going.\n","\n","Let's start."]},{"cell_type":"markdown","metadata":{"id":"NYAzBeUhJwQg"},"source":["### Part 1: Making Predictions\n","\n","(a) We're going to use predictions made by our model as a guide for tuning the coefficients, so we need a function that can make predictions. This *same* function can also apply the final coefficients we have learned to make predictions over new data. \n","\n","Write a function called that takes a **single instance** (a list of input features, from for example an X_train or X_test data set), and a list of coefficients, and calculates the predicted y value, using the formula:\n","\n","y = b0 + b1*x1 + b2*x2 + ... + bN*xN\n","\n","This should work for ANY length of instance, but we can always assume that the length of the instance list and the length of the coefficent list are different by one. Why one? For the instance, imagine that the input feature values of a single instance are:\n","\n","[x1,x2,x3] \n","\n","For the coefficients, let's assume that the list contains all the coefficients, including b0. Let's also assume that we ALWAYS store the b0 coefficient at position coefficients[0] (i.e. the first position in the coefficients list).\n","\n","The resulting coefficient list for the above instance would then be:\n","\n","[b0,b1,b2,b3]\n","\n","Again, your code should work for ANY length of instance, and the corresponding list of coefficients.\n","\n","Your function should return the predicted y value for a given instance and a given set of coefficients, using the formula above. \n","\n","(b) In the Simple Linear Regression assignment, you applied your model to a contrived data set. I've reproduced this data set below. Go through this data set one instance at a time and call your new predict function for each instance. You can use the coefficients [0.4, 0.8], which are almost exactly what you learned as coefficients in Assignment 2. \n","\n","For each instance, print out the correct value and the value predicted by your function from (a). You should see that it performs reasonably well - the predicted values should be close to but not exactly the same as the actual values.\n","\n","I'd take the time to print nicely here. It helps."]},{"cell_type":"code","metadata":{"id":"MztlfcIMJwQh","executionInfo":{"status":"ok","timestamp":1681418577694,"user_tz":240,"elapsed":6,"user":{"displayName":"Manav Bilakhia","userId":"01301071607022221367"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"b13fb2cb-cf4d-4d0b-d34f-2e60b5edadce"},"source":["\n","# Write your predict function here\n","def predict(instance, coefficients):\n","    y = coefficients[0]\n","    for i in range(len(instance)-1):\n","      y += coefficients[i+1] * instance[i]\n","    return y\n","\n","# Apply your function to the contrived dataset\n","\n","dataset = [[1,1],[2,3],[4,3],[3,2],[5,5]]\n","coef = [0.4,0.8]\n","\n","# Split your data into X_train and y_train\n","X_train = [[row[0]] for row in dataset]\n","y_train = [row[1] for row in dataset]\n","\n","print(X_train)\n","print(y_train)\n","# In this assignment, it's important that X_train be a list of lists, i.e.\n","# [[1],[2],[4],[3],[5]]\n","# in this case\n","# Go through each instance of X_train, and get a predicted y value\n","# Print out the predicted y, and the corresponding actual y from y_train\n","\n","for instance in dataset:\n","    y_predicted = predict(instance, coef)\n","    print(f\"Actual: {instance[1]} \\t Predicted: {y_predicted}\")"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[[1], [2], [4], [3], [5]]\n","[1, 3, 3, 2, 5]\n","Actual: 1 \t Predicted: 1.2000000000000002\n","Actual: 3 \t Predicted: 2.0\n","Actual: 3 \t Predicted: 3.6\n","Actual: 2 \t Predicted: 2.8000000000000003\n","Actual: 5 \t Predicted: 4.4\n"]}]},{"cell_type":"markdown","metadata":{"id":"LR-uYV08JwQl"},"source":["### Part 2: Learning coefficients with Stochastic Gradient Descent\n","\n","We now need to estimate coefficients for ourselves by writing a stochastic gradient descent function. For a given learning rate, for a number of iterations (epochs), we're going to estimate our coefficients.  Given a set of training data, spilt into X_train and y_train, we're going to:\n","\n","- loop over each epoch (where we go through ALL the instances of the training data)\n","- loop over each row (instance) in the training data, for an epoch\n","- loop over each coefficient, for each feature in each instance, for each epoch\n","\n","As computer scientists you should recognize that this requires three, nested for loops, and you should have a sense (a big-O kind of sense) why this can take a long time for large data sets.\n","\n","Coefficients are updated based on the error the model made. The error is calculated as the difference between the predicted y value and the actual y value:\n","\n","error = prediction - actual\n","\n","There is one coefficient for EACH input attribute, and these are updated every time, for example:\n","\n","b1 = b1 - learning_rate * error * x1\n","\n","We ALSO need to update the special intercept coefficient b0:\n","\n","b0 = b0 - learning_rate * error\n","\n","(c) Implement the following algorithm for Stochastic Gradient Descent, that takes a list of X_train values, the list of corresponding y_train values, and two other parameters, learning_rate and epochs.\n","\n","The algorithm is as follows: \n","\n","- initialize a list for the output coefficients. The length of the list will be the same as the length of each instance in your X_train data, + 1 for the special b0 coefficient. We can initialize all the coefficients to 0.0 to start with (remember, it doesn't really matter where we start)\n","- for each epoch\n","    - initialize the total error to 0\n","    - for each instance in the X_train data\n","        1. calculate the prediction for that instance, given the current list of coefficients, using our function from (a)\n","        2. calculate the error for that prediction, using the corresponding y_train value \n","        3. square the error, and add it to the total error. Note that squaring the individual error means it will always be a positive value. ALSO NOTE: We don't use this squared error for updating the coefficients - we use the error calculated in **step 2**. This squaring is just to give us nice, readable output at the end of each epoch. \n","        4. Now update the coefficients, using the formulas given above. One update for b0 (which should always be at position 0 in the coefficients list), and then loop over the remaining coefficients, performing an update for each one, b1 through bN.\n","        \n","    - At the end of each epoch, print out the epoch number (we can start at epoch 0), the learning rate, and the total error for this epoch (DO THIS ON ONE LINE FOR READABILITY).\n","    - repeat for the next epoch\n","- once we've iterated through all the epochs, return the list of coefficients\n","\n","(d) Apply your stochastic gradient descent function to the contrived dataset, given below. If it's working, you should see the error rate falling each epoch. You should also note that the value of the coefficients learned at the end isn't quite the same as Simple Linear Regression, because we're estimating this time. You could try learning longer (more epochs), or altering the learning rate, and see if the coefficients approach the optimal values we learned in Assignment 2.\n","\n","I've given you TWO datasets to work with. The first aligns with last week, so you can test your results. The second involves more features, so you can make sure you haven't accidentally made coding decisions that rely on only two features."]},{"cell_type":"code","metadata":{"id":"PsT2NDwiJwQm","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1681420021693,"user_tz":240,"elapsed":198,"user":{"displayName":"Manav Bilakhia","userId":"01301071607022221367"}},"outputId":"e5c48bdf-21f0-4907-b4b2-802f1e2485b2"},"source":["# Write your coefficientsSGD(train,test,learning_rate,epochs) here\n","\n","def coefficientsSGD(train, learning_rate, epochs):  # there was some ambiguity here hence I changed the function signature. In the comments above it said to have \" X_train values, the list of corresponding y_train values, and two other parameters, learning_rate and epochs.\"\n","# but the function signature youy gave us had params (train,test,learning_rate,epochs). So i zipped the xtrain and Y train in train and removed the test param\n","    coefficients = [0.0 for i in range(len(train[0]))]\n","    for epoch in range(epochs):\n","        total_error = 0\n","        for row in train:\n","            y = predict(row, coefficients)\n","            error = y - row[-1]\n","            total_error += error**2\n","            coefficients[0] = coefficients[0] - learning_rate * error\n","            for i in range(len(row)-1):\n","                coefficients[i+1] = coefficients[i+1] - learning_rate * error * row[i]\n","        print(\"Epoch=%d, learning_rate=%.3f, error=%.3f\" % (epoch, learning_rate, total_error))\n","    return coefficients\n","\n","# Apply to the contrived data here. Try my parameters first, before you experiment\n","# BEFORE you submit your assigment, return the code to using my parameters:\n","# learning_rate = 0.001\n","# epochs = 50\n","\n","dataset1 = [[1,1],[2,3],[4,3],[3,2],[5,5]]\n","dataset2 = [[1,2,1],[2,2,3],[4,3,3],[3,2,2],[5,4,5],[5,2,6],[4,5,4],[7,5,5]]\n","\n","\n","learning_rate = 0.001\n","epochs = 50\n","\n","# Slice your data into X_train and y_train\n","# Call your SGD function to obtain a list of coefficients\n","# PRINT your coefficients nicely\n","\n","X_train = [row[:-1] for row in dataset1]\n","y_train = [row[-1] for row in dataset1]\n","\n","coefficients = coefficientsSGD(dataset1, learning_rate, epochs)\n","print(\"Coefficients: \", coefficients)\n","\n","X_train = [row[:-1] for row in dataset2]\n","y_train = [row[-1] for row in dataset2]\n","\n","coefficients = coefficientsSGD(dataset2, learning_rate, epochs)\n","print(\"Coefficients: \", coefficients)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch=0, learning_rate=0.001, error=46.236\n","Epoch=1, learning_rate=0.001, error=41.305\n","Epoch=2, learning_rate=0.001, error=36.930\n","Epoch=3, learning_rate=0.001, error=33.047\n","Epoch=4, learning_rate=0.001, error=29.601\n","Epoch=5, learning_rate=0.001, error=26.543\n","Epoch=6, learning_rate=0.001, error=23.830\n","Epoch=7, learning_rate=0.001, error=21.422\n","Epoch=8, learning_rate=0.001, error=19.285\n","Epoch=9, learning_rate=0.001, error=17.389\n","Epoch=10, learning_rate=0.001, error=15.706\n","Epoch=11, learning_rate=0.001, error=14.213\n","Epoch=12, learning_rate=0.001, error=12.888\n","Epoch=13, learning_rate=0.001, error=11.712\n","Epoch=14, learning_rate=0.001, error=10.668\n","Epoch=15, learning_rate=0.001, error=9.742\n","Epoch=16, learning_rate=0.001, error=8.921\n","Epoch=17, learning_rate=0.001, error=8.191\n","Epoch=18, learning_rate=0.001, error=7.544\n","Epoch=19, learning_rate=0.001, error=6.970\n","Epoch=20, learning_rate=0.001, error=6.461\n","Epoch=21, learning_rate=0.001, error=6.009\n","Epoch=22, learning_rate=0.001, error=5.607\n","Epoch=23, learning_rate=0.001, error=5.251\n","Epoch=24, learning_rate=0.001, error=4.935\n","Epoch=25, learning_rate=0.001, error=4.655\n","Epoch=26, learning_rate=0.001, error=4.406\n","Epoch=27, learning_rate=0.001, error=4.186\n","Epoch=28, learning_rate=0.001, error=3.990\n","Epoch=29, learning_rate=0.001, error=3.816\n","Epoch=30, learning_rate=0.001, error=3.662\n","Epoch=31, learning_rate=0.001, error=3.525\n","Epoch=32, learning_rate=0.001, error=3.404\n","Epoch=33, learning_rate=0.001, error=3.296\n","Epoch=34, learning_rate=0.001, error=3.200\n","Epoch=35, learning_rate=0.001, error=3.115\n","Epoch=36, learning_rate=0.001, error=3.040\n","Epoch=37, learning_rate=0.001, error=2.973\n","Epoch=38, learning_rate=0.001, error=2.914\n","Epoch=39, learning_rate=0.001, error=2.862\n","Epoch=40, learning_rate=0.001, error=2.815\n","Epoch=41, learning_rate=0.001, error=2.773\n","Epoch=42, learning_rate=0.001, error=2.737\n","Epoch=43, learning_rate=0.001, error=2.704\n","Epoch=44, learning_rate=0.001, error=2.675\n","Epoch=45, learning_rate=0.001, error=2.650\n","Epoch=46, learning_rate=0.001, error=2.627\n","Epoch=47, learning_rate=0.001, error=2.607\n","Epoch=48, learning_rate=0.001, error=2.589\n","Epoch=49, learning_rate=0.001, error=2.573\n","Coefficients:  [0.22998234937311363, 0.8017220304137576]\n","Epoch=0, learning_rate=0.001, error=104.960\n","Epoch=1, learning_rate=0.001, error=68.133\n","Epoch=2, learning_rate=0.001, error=45.434\n","Epoch=3, learning_rate=0.001, error=31.435\n","Epoch=4, learning_rate=0.001, error=22.793\n","Epoch=5, learning_rate=0.001, error=17.452\n","Epoch=6, learning_rate=0.001, error=14.144\n","Epoch=7, learning_rate=0.001, error=12.091\n","Epoch=8, learning_rate=0.001, error=10.810\n","Epoch=9, learning_rate=0.001, error=10.008\n","Epoch=10, learning_rate=0.001, error=9.500\n","Epoch=11, learning_rate=0.001, error=9.174\n","Epoch=12, learning_rate=0.001, error=8.962\n","Epoch=13, learning_rate=0.001, error=8.820\n","Epoch=14, learning_rate=0.001, error=8.722\n","Epoch=15, learning_rate=0.001, error=8.652\n","Epoch=16, learning_rate=0.001, error=8.598\n","Epoch=17, learning_rate=0.001, error=8.555\n","Epoch=18, learning_rate=0.001, error=8.520\n","Epoch=19, learning_rate=0.001, error=8.489\n","Epoch=20, learning_rate=0.001, error=8.461\n","Epoch=21, learning_rate=0.001, error=8.434\n","Epoch=22, learning_rate=0.001, error=8.410\n","Epoch=23, learning_rate=0.001, error=8.386\n","Epoch=24, learning_rate=0.001, error=8.363\n","Epoch=25, learning_rate=0.001, error=8.340\n","Epoch=26, learning_rate=0.001, error=8.318\n","Epoch=27, learning_rate=0.001, error=8.297\n","Epoch=28, learning_rate=0.001, error=8.276\n","Epoch=29, learning_rate=0.001, error=8.255\n","Epoch=30, learning_rate=0.001, error=8.234\n","Epoch=31, learning_rate=0.001, error=8.214\n","Epoch=32, learning_rate=0.001, error=8.194\n","Epoch=33, learning_rate=0.001, error=8.174\n","Epoch=34, learning_rate=0.001, error=8.154\n","Epoch=35, learning_rate=0.001, error=8.135\n","Epoch=36, learning_rate=0.001, error=8.115\n","Epoch=37, learning_rate=0.001, error=8.096\n","Epoch=38, learning_rate=0.001, error=8.078\n","Epoch=39, learning_rate=0.001, error=8.059\n","Epoch=40, learning_rate=0.001, error=8.041\n","Epoch=41, learning_rate=0.001, error=8.022\n","Epoch=42, learning_rate=0.001, error=8.004\n","Epoch=43, learning_rate=0.001, error=7.987\n","Epoch=44, learning_rate=0.001, error=7.969\n","Epoch=45, learning_rate=0.001, error=7.952\n","Epoch=46, learning_rate=0.001, error=7.934\n","Epoch=47, learning_rate=0.001, error=7.917\n","Epoch=48, learning_rate=0.001, error=7.900\n","Epoch=49, learning_rate=0.001, error=7.884\n","Coefficients:  [0.15311017505222785, 0.6342385233806282, 0.2923092415366756]\n"]}]},{"cell_type":"markdown","metadata":{"id":"IMJDSWn1JwQr"},"source":["(e) Now you have sufficient functionality to write a function to learn and make predictions using multivariate linear regression. \n","\n","Create a function that takes:\n","\n","1. X_train\n","2. y_train\n","\n","which are the training data, from which we'll estimate coefficients\n","\n","3. X_test\n","\n","our test data, which we'll use to evaluate the model\n","\n","4. Learning rate\n","5. Epochs\n","\n","and our parameters for learning.\n","\n","Just like the last assignment, we're going to use the same dataset here for both training and testing, even though that might not be a great idea (why not?).\n","\n","Here's the multivariate linear regression algorithm. We're going to estimate our coefficients from the training data, using the function from (c) above. We're going to create a new list, to hold our predictions. Then for each entry in the testing data, we're going to read the input value, and make a prediction, using our function from (a). We'll append our predicted y value to the prediction list. We're going to return our list of predictions.\n","\n","(f) Compare the predicitons to the actual y values stored in y_test, and compute the RMSE value. Print it nicely.\n","Also print the zeroR value. This means you're going to need your RMSE function and your zeroRR function from the previous assignment."]},{"cell_type":"code","metadata":{"id":"YvhXcBGsJwQs","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1681405151907,"user_tz":240,"elapsed":155,"user":{"displayName":"Manav Bilakhia","userId":"01301071607022221367"}},"outputId":"91c37588-f7b9-42b1-e4e8-d29cbdff86e0"},"source":["# Write your function for multivariate linear regression here\n","import math\n","\n","# function to perform multivariate linear regression\n","def multivariate_linear_regression(X_train, y_train, X_test, learning_rate, epochs):\n","    # obtain the list of coefficients from the training data\n","    train = [(a[0], b) for a, b in zip(X_train, y_train)]\n","    coefficients = coefficientsSGD(train, learning_rate, epochs)\n","    \n","    # make predictions on the test data using the obtained coefficients\n","    predictions = []\n","    for instance in X_test:\n","        prediction = predict(instance, coefficients)\n","        predictions.append(prediction)\n","    \n","    # return the list of predictions\n","    return predictions\n","\n","# function to compute RMSE value\n","def compute_rmse(predictions, actual):\n","    n = len(predictions)\n","    sum_squared_error = 0\n","    for i in range(n):\n","        error = predictions[i] - actual[i]\n","        sum_squared_error += error**2\n","    mean_squared_error = sum_squared_error / n\n","    rmse = math.sqrt(mean_squared_error)\n","    return rmse\n","\n","# function to compute zeroR value\n","def compute_zeroR(actual):\n","    value_counts = {}\n","    for value in actual:\n","        if value in value_counts:\n","            value_counts[value] += 1\n","        else:\n","            value_counts[value] = 1\n","    most_common_value = max(value_counts, key=value_counts.get)\n","    zeroR = value_counts[most_common_value] / len(actual)\n","    return zeroR\n","\n","dataset1 = [[1,1],[2,3],[4,3],[3,2],[5,5]]\n","dataset2 = [[1,2,1],[2,2,3],[4,3,3],[3,2,2],[5,4,5],[5,2,6],[4,5,4],[7,5,5]]\n","\n","\n","learning_rate = 0.001\n","epochs = 50\n","\n","# Slice your data into X_train and y_train\n","# Call your SGD function to obtain a list of coefficients\n","# PRINT your coefficients nicely\n","\n","print (\"-----dataset1-----\")\n","X_train = [row[:-1] for row in dataset1]\n","y_train = [row[-1] for row in dataset1]\n","X_test = [row[:-1] for row in dataset1]\n","\n","\n","# obtain the list of predictions\n","predictions = multivariate_linear_regression(X_train, y_train, X_test, learning_rate, epochs)\n","\n","# compute the RMSE and zeroR values\n","actual = [row[-1] for row in dataset1]\n","rmse = compute_rmse(predictions, actual)\n","zeroR = compute_zeroR(actual)\n","\n","# print the results\n","print(\"RMSE: {:.2f}\".format(rmse))\n","print(\"zeroR: {:.2f}\".format(zeroR))\n","\n","print (\"-----dataset2-----\")\n","X_train = [row[:-1] for row in dataset2]\n","y_train = [row[-1] for row in dataset2]\n","X_test = [row[:-1] for row in dataset2]\n","\n","\n","# obtain the list of predictions\n","predictions = multivariate_linear_regression(X_train, y_train, X_test, learning_rate, epochs)\n","\n","# compute the RMSE and zeroR values\n","actual = [row[-1] for row in dataset2]\n","rmse = compute_rmse(predictions, actual)\n","zeroR = compute_zeroR(actual)\n","\n","# print the results\n","print(\"RMSE: {:.2f}\".format(rmse))\n","print(\"zeroR: {:.2f}\".format(zeroR))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["-----dataset1-----\n","Epoch=0, learning_rate=0.001, error=46.236\n","Epoch=1, learning_rate=0.001, error=41.305\n","Epoch=2, learning_rate=0.001, error=36.930\n","Epoch=3, learning_rate=0.001, error=33.047\n","Epoch=4, learning_rate=0.001, error=29.601\n","Epoch=5, learning_rate=0.001, error=26.543\n","Epoch=6, learning_rate=0.001, error=23.830\n","Epoch=7, learning_rate=0.001, error=21.422\n","Epoch=8, learning_rate=0.001, error=19.285\n","Epoch=9, learning_rate=0.001, error=17.389\n","Epoch=10, learning_rate=0.001, error=15.706\n","Epoch=11, learning_rate=0.001, error=14.213\n","Epoch=12, learning_rate=0.001, error=12.888\n","Epoch=13, learning_rate=0.001, error=11.712\n","Epoch=14, learning_rate=0.001, error=10.668\n","Epoch=15, learning_rate=0.001, error=9.742\n","Epoch=16, learning_rate=0.001, error=8.921\n","Epoch=17, learning_rate=0.001, error=8.191\n","Epoch=18, learning_rate=0.001, error=7.544\n","Epoch=19, learning_rate=0.001, error=6.970\n","Epoch=20, learning_rate=0.001, error=6.461\n","Epoch=21, learning_rate=0.001, error=6.009\n","Epoch=22, learning_rate=0.001, error=5.607\n","Epoch=23, learning_rate=0.001, error=5.251\n","Epoch=24, learning_rate=0.001, error=4.935\n","Epoch=25, learning_rate=0.001, error=4.655\n","Epoch=26, learning_rate=0.001, error=4.406\n","Epoch=27, learning_rate=0.001, error=4.186\n","Epoch=28, learning_rate=0.001, error=3.990\n","Epoch=29, learning_rate=0.001, error=3.816\n","Epoch=30, learning_rate=0.001, error=3.662\n","Epoch=31, learning_rate=0.001, error=3.525\n","Epoch=32, learning_rate=0.001, error=3.404\n","Epoch=33, learning_rate=0.001, error=3.296\n","Epoch=34, learning_rate=0.001, error=3.200\n","Epoch=35, learning_rate=0.001, error=3.115\n","Epoch=36, learning_rate=0.001, error=3.040\n","Epoch=37, learning_rate=0.001, error=2.973\n","Epoch=38, learning_rate=0.001, error=2.914\n","Epoch=39, learning_rate=0.001, error=2.862\n","Epoch=40, learning_rate=0.001, error=2.815\n","Epoch=41, learning_rate=0.001, error=2.773\n","Epoch=42, learning_rate=0.001, error=2.737\n","Epoch=43, learning_rate=0.001, error=2.704\n","Epoch=44, learning_rate=0.001, error=2.675\n","Epoch=45, learning_rate=0.001, error=2.650\n","Epoch=46, learning_rate=0.001, error=2.627\n","Epoch=47, learning_rate=0.001, error=2.607\n","Epoch=48, learning_rate=0.001, error=2.589\n","Epoch=49, learning_rate=0.001, error=2.573\n","RMSE: 2.89\n","zeroR: 0.40\n","-----dataset2-----\n","Epoch=0, learning_rate=0.001, error=111.412\n","Epoch=1, learning_rate=0.001, error=83.660\n","Epoch=2, learning_rate=0.001, error=63.255\n","Epoch=3, learning_rate=0.001, error=48.252\n","Epoch=4, learning_rate=0.001, error=37.219\n","Epoch=5, learning_rate=0.001, error=29.105\n","Epoch=6, learning_rate=0.001, error=23.136\n","Epoch=7, learning_rate=0.001, error=18.746\n","Epoch=8, learning_rate=0.001, error=15.515\n","Epoch=9, learning_rate=0.001, error=13.137\n","Epoch=10, learning_rate=0.001, error=11.387\n","Epoch=11, learning_rate=0.001, error=10.098\n","Epoch=12, learning_rate=0.001, error=9.149\n","Epoch=13, learning_rate=0.001, error=8.449\n","Epoch=14, learning_rate=0.001, error=7.933\n","Epoch=15, learning_rate=0.001, error=7.552\n","Epoch=16, learning_rate=0.001, error=7.271\n","Epoch=17, learning_rate=0.001, error=7.063\n","Epoch=18, learning_rate=0.001, error=6.909\n","Epoch=19, learning_rate=0.001, error=6.795\n","Epoch=20, learning_rate=0.001, error=6.711\n","Epoch=21, learning_rate=0.001, error=6.648\n","Epoch=22, learning_rate=0.001, error=6.601\n","Epoch=23, learning_rate=0.001, error=6.566\n","Epoch=24, learning_rate=0.001, error=6.539\n","Epoch=25, learning_rate=0.001, error=6.519\n","Epoch=26, learning_rate=0.001, error=6.504\n","Epoch=27, learning_rate=0.001, error=6.492\n","Epoch=28, learning_rate=0.001, error=6.483\n","Epoch=29, learning_rate=0.001, error=6.476\n","Epoch=30, learning_rate=0.001, error=6.471\n","Epoch=31, learning_rate=0.001, error=6.466\n","Epoch=32, learning_rate=0.001, error=6.463\n","Epoch=33, learning_rate=0.001, error=6.460\n","Epoch=34, learning_rate=0.001, error=6.457\n","Epoch=35, learning_rate=0.001, error=6.455\n","Epoch=36, learning_rate=0.001, error=6.453\n","Epoch=37, learning_rate=0.001, error=6.451\n","Epoch=38, learning_rate=0.001, error=6.450\n","Epoch=39, learning_rate=0.001, error=6.448\n","Epoch=40, learning_rate=0.001, error=6.447\n","Epoch=41, learning_rate=0.001, error=6.446\n","Epoch=42, learning_rate=0.001, error=6.445\n","Epoch=43, learning_rate=0.001, error=6.443\n","Epoch=44, learning_rate=0.001, error=6.442\n","Epoch=45, learning_rate=0.001, error=6.441\n","Epoch=46, learning_rate=0.001, error=6.440\n","Epoch=47, learning_rate=0.001, error=6.439\n","Epoch=48, learning_rate=0.001, error=6.438\n","Epoch=49, learning_rate=0.001, error=6.437\n","RMSE: 0.89\n","zeroR: 0.25\n"]}]},{"cell_type":"markdown","metadata":{"id":"d6km1GgQJwQv"},"source":["### Part 3: Introduction to scikit-learn\n","\n","\n","As we go through this class I'll introduce you to some of the functionality of scikit learn. \n","\n","Below I import the Stochastic Gradient Descent regressor. This learns the coefficients using SGD, and then when predict is called uses those coefficients in the multivariate linear regression formula.\n","\n","Note that when I create an instance of the SGDRegressor, I give a parameter which is the MAXIMUM number of iterations before stopping. The implementation of SGD in scikit learn is a little more advanced than what you did above, in that it will AUTOMATICALLY stop when the amount of learning (the change in error) stops changing significantly enough. \n","\n","I've added some code to print out the intercept (b0), the list of input coefficients, AND the number of iterations before it stopped. \n","\n","Recreate the four lists X_train, y_train, X_test and y_test as you would have for the previous examples, using the same data for both training and testing. Then the following code should run. \n","\n","Note that I have to do some reshaping, to make them into 2D arrays from the 1D list that python uses. This is just a requirement of scikit-learn, and you can see how I do it below.\n","\n","You can find out more in general at: https://scikit-learn.org/stable/index.html\n","\n","This time however, I want you to add the following:\n","- Add in the linear regression from sklearn from Assignment 2(* see the note below)\n","- Add in the dummy regressor (zeroR)\n","- Add in RMSE evaluation for all three models (SGDRegressor, Linear Regression and zeroR)\n","- Print out the RMSE for all three\n","- Add a text box AFTER the results commenting on which model(s) are better and why\n","- As a last thing, CHANGE the max iterations in the SGD regressor to 20. You SHOULD get a warning when you run it about the model NOT converging. This means it did NOT have enough 'time' to find the best solution.\n","\n","Think carefully about structuring your code, and presenting the output. Make it neat and tidy. Consider this a REPORT as well as a coding assignment. \n","\n","(\\*) Last week you used the built in LinearRegression model to do SIMPLE linear regressrion. By default, the linear regression function uses least squares to estimate the coefficients, which means it works for multivariate linear regression too, by doing matrix math. \n","\n","So it's using a different estimation approach to acquire the same coefficients. You could print out THOSE coefficients and see if they are similar to the ones from SGD. There are OTHER ML models (most notably, any neural approach) where least squares DOES NOT work anymore, and we MUST use SGD (or some varient).\n","\n","You can read more here:\n","* [SGDRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDRegressor.html)\n","* [LinearRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html)\n","* [zeroR](https://scikit-learn.org/stable/modules/generated/sklearn.dummy.DummyRegressor.html)\n","* [Mean Squared Error](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_squared_error.html)\n","\n"]},{"cell_type":"code","metadata":{"id":"yvH3q0vxJwQw","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1681318761171,"user_tz":240,"elapsed":1327,"user":{"displayName":"Manav Bilakhia","userId":"01301071607022221367"}},"outputId":"6e6de672-f1e9-4b50-dadc-ecc77e6679ab"},"source":["import numpy as np\n","from sklearn.linear_model import SGDRegressor\n","from sklearn.linear_model import LinearRegression\n","from sklearn.dummy import DummyRegressor\n","from sklearn.metrics import mean_squared_error\n","\n","# Use this dataset for these examples\n","\n","dataset = [[1,2,1],[2,2,3],[4,3,3],[3,2,2],[5,4,5],[5,2,6],[4,5,4],[7,5,5]]\n","\n","\n","# Create and shape our data\n","# **************************************************************************\n","# CREATE YOUR X_train,y_train, X_Test,y_test datasets here\n","# As before, you should use the same data for X_train and X_test\n","# AND y_train and y_test - although you can ALWAYS test your SGD against the \n","# dataset I gave you earlier\n","X_train = [row[:-1] for row in dataset]\n","y_train = [row[-1] for row in dataset]\n","X_test = [row[:-1] for row in dataset]\n","y_test = [row[-1] for row in dataset]\n","\n","# **************************************************************************\n","# Create an instance of the SGD Regressor\n","# Create a model, by fitting the X_train values to the y_train values\n","\n","lin_reg = LinearRegression()\n","zero_r = DummyRegressor(strategy=\"mean\")\n","sgd_clf =SGDRegressor(max_iter=20)\n","#sgd_clf =SGDRegressor(max_iter=200)\n","sgd = sgd_clf.fit(X_train, y_train)\n","lin_reg.fit(X_train, y_train)\n","zero_r.fit(X_train, y_train)\n","\n","# Explore some built in attributes\n","# Note: even though we set max_iter to 200\n","# This implementation stops when the learning doesn't improve \"enough\"\n","\n","# Test our model on the X_test data\n","\n","# Calculate the RMSE for the SGD Regressor model\n","sgd_predY = sgd.predict(X_test)\n","sgd_rmse = np.sqrt(mean_squared_error(y_test, sgd_predY))\n","\n","# Calculate the RMSE for the Linear Regression model\n","lin_predY = lin_reg.predict(X_test)\n","lin_rmse = np.sqrt(mean_squared_error(y_test, lin_predY))\n","\n","# Calculate the RMSE for the Dummy Regressor model\n","zero_predY = zero_r.predict(X_test)\n","zero_rmse = np.sqrt(mean_squared_error(y_test, zero_predY))\n","\n","\n","# Print out predicted vs. actual\n","print(\"SGDRegressor\")\n","print('INTERCEPT:',sgd.intercept_)\n","print('COEFFS:',sgd.coef_)\n","print('ITERATIONS TAKEN:',sgd.n_iter_)\n","print('\\nPREDICTED : ACTUAL')\n","print('------------------')\n","for i in range(len(sgd_predY)):\n","  print('    {:.2f}  :  {:.2f}  '.format(sgd_predY[i],y_test[i]))\n","print()\n","\n","print(\"Linear Regression\")\n","print('INTERCEPT:',lin_reg.intercept_)\n","print('COEFFS:',lin_reg.coef_)\n","#print('ITERATIONS TAKEN:',lin_reg.n_iter_)\n","print('\\nPREDICTED : ACTUAL')\n","print('------------------')\n","for i in range(len(lin_predY)):\n","  print('    {:.2f}  :  {:.2f}  '.format(lin_predY[i],y_test[i]))\n","print()\n","\n","print(\"Dummy Regressor\")\n","#print('INTERCEPT:',zero_r.intercept_)\n","#print('COEFFS:',zero_r.coef_)\n","#print('ITERATIONS TAKEN:',zero_r.n_iter_)\n","print('\\nPREDICTED : ACTUAL')\n","print('------------------')\n","for i in range(len(zero_predY)):\n","  print('    {:.2f}  :  {:.2f}  '.format(zero_predY[i],y_test[i]))\n","print()\n","\n","print(\"RMSE for SGD Regressor: {:.2f}\".format(sgd_rmse))\n","print(\"RMSE for Linear Regression: {:.2f}\".format(lin_rmse))\n","print(\"RMSE for Dummy Regressor: {:.2f}\".format(zero_rmse))\n","\n","# Add a text box commenting on which model(s) are better and why\n","print(\"\\nBased on the RMSE values, the Linear Regression model performs the best with the lowest RMSE value. The SGD Regressor model performs slightly worse than the Linear Regression model, but still better than the Dummy Regressor model. The Dummy Regressor model simply predicts the mean value of the training set, so it is not surprising that it has the highest RMSE value.\")"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["SGDRegressor\n","INTERCEPT: [0.18418831]\n","COEFFS: [0.65678322 0.22638872]\n","ITERATIONS TAKEN: 20\n","\n","PREDICTED : ACTUAL\n","------------------\n","    1.29  :  1.00  \n","    1.95  :  3.00  \n","    3.49  :  3.00  \n","    2.61  :  2.00  \n","    4.37  :  5.00  \n","    3.92  :  6.00  \n","    3.94  :  4.00  \n","    5.91  :  5.00  \n","\n","Linear Regression\n","INTERCEPT: 1.0259740259740249\n","COEFFS: [ 0.86796537 -0.24458874]\n","\n","PREDICTED : ACTUAL\n","------------------\n","    1.40  :  1.00  \n","    2.27  :  3.00  \n","    3.76  :  3.00  \n","    3.14  :  2.00  \n","    4.39  :  5.00  \n","    4.88  :  6.00  \n","    3.27  :  4.00  \n","    5.88  :  5.00  \n","\n","Dummy Regressor\n","\n","PREDICTED : ACTUAL\n","------------------\n","    3.62  :  1.00  \n","    3.62  :  3.00  \n","    3.62  :  3.00  \n","    3.62  :  2.00  \n","    3.62  :  5.00  \n","    3.62  :  6.00  \n","    3.62  :  4.00  \n","    3.62  :  5.00  \n","\n","RMSE for SGD Regressor: 0.96\n","RMSE for Linear Regression: 0.83\n","RMSE for Dummy Regressor: 1.58\n","\n","Based on the RMSE values, the Linear Regression model performs the best with the lowest RMSE value. The SGD Regressor model performs slightly worse than the Linear Regression model, but still better than the Dummy Regressor model. The Dummy Regressor model simply predicts the mean value of the training set, so it is not surprising that it has the highest RMSE value.\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_stochastic_gradient.py:1548: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"6onOoKhGCWR3"},"execution_count":null,"outputs":[]}]}