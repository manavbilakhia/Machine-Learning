{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.8"},"colab":{"provenance":[{"file_id":"19hWGgIjQuVpe5uQM_njKul3ueXMyAhxP","timestamp":1682352691141},{"file_id":"1XGeG7MdrpY2vn4_UpspFYBrBfWukEiUB","timestamp":1588017449903}]}},"cells":[{"cell_type":"markdown","metadata":{"id":"NdO0ZhXOvwGa"},"source":["# CSC-321: Data Mining and Machine Learning\n","# Manav Bilakhia\n","\n","## Assignment 5: Working with real data\n","\n","For this notebook, you should use your own implementation of ML code. That means use your own functions for SLR, MLR, SGD, , Logistic Regression, zeroR etc. I will tell you if you should use scikit implementations.\n","\n","**FORMATTING NOTE:** What follows is a series of experiements, using different datasets, with different numbers of instances and features. You're going to end up with a long notebook containing all kinds of print statements and cells containing numbers. It is VERY easy to get confused. I strongly suggest the following:\n","- minimize output. This is tricky, given that you should ALWAYS do sanity checks on your data (using functions like .info(),.head() and .describe() that I introduce below). I recommend performing them, but then comment out the print statement. Once you've checked your data you don't have to always display it.\n","- Keep what output you have COMPACT. Don't overdo it, squashing things up together, but be mindful of spacing, like I suggested with the output of your SGD function. \n","- Feel free, once you are convinced things like SGD work, to replace the constant output of epoch/learning rate etc. I edit mine to print just TWICE. Once at the start (epoch 0), and once at the end. This isn't optimal, but I'll explain why later.\n","- ALWAYS print the following information at the top of any cell when you run an experiment: Data set name, number of instances, number of features\n","- ALWAYS include algorithm name when printing a result. \n","\n","In grading your notebook, I WILL BE ATTENTIVE TO LAYOUT AND PRESENTATION OF RESULTS. If you're just dumping numbers to the screen you're going to have a bad time.\n","\n","There are cells below where you have to edit things, or write things, and there are cells where you have to just run them, and observe the results. I recommend running this notebook one cell at a time, and making sure you are clear on what the cell is doing, and why.\n","\n","### Part 1: Loading Data\n","\n","We're going to make use of the pandas library (because pandas are cute) to load data and to get a quick overview. For the first data set, I'm going to walk you through some typical commands. Note that in your work, you don't always have to print these out, it's just good practice as you're working through a problem. I have lost track of the number of times I have sliced a data set, and left behind more instances or features than I intended to. \n","\n","For reference:\n","- [Pandas](https://pandas.pydata.org/)\n","\n","(*)delete as appropriate\n","\n","Run the following cell."]},{"cell_type":"code","metadata":{"id":"kGrXgUsQvwGb","colab":{"base_uri":"https://localhost:8080/","height":206},"executionInfo":{"status":"ok","timestamp":1682635541032,"user_tz":240,"elapsed":308,"user":{"displayName":"Manav Bilakhia","userId":"01301071607022221367"}},"outputId":"03d5ca3b-fdfb-4cf7-ae64-8086a34d0883"},"source":["%matplotlib inline\n","import pandas as pd\n","\n","# Load the data\n","\n","insurance_data = pd.read_csv('https://raw.githubusercontent.com/nixwebb/CSV_Data/master/insurance.csv')\n","\n","# Show the head - just the first 5 entries\n","\n","insurance_data.head()\n"],"execution_count":11,"outputs":[{"output_type":"execute_result","data":{"text/plain":["   108  392.5\n","0   19   46.2\n","1   13   15.7\n","2  124  422.2\n","3   40  119.4\n","4   57  170.9"],"text/html":["\n","  <div id=\"df-ecfa421c-619b-44a6-866f-1f4bb7b87911\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>108</th>\n","      <th>392.5</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>19</td>\n","      <td>46.2</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>13</td>\n","      <td>15.7</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>124</td>\n","      <td>422.2</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>40</td>\n","      <td>119.4</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>57</td>\n","      <td>170.9</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ecfa421c-619b-44a6-866f-1f4bb7b87911')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-ecfa421c-619b-44a6-866f-1f4bb7b87911 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-ecfa421c-619b-44a6-866f-1f4bb7b87911');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":11}]},{"cell_type":"markdown","metadata":{"id":"MJHgLalGdZPI"},"source":["Look at the data above. The column headings should look weird. That's because pandas decided to treat the first row as column headings. I can prevent that by supplying my own column headings, which I demonstrate below. \n","\n","I chose X and y. While this is illustrative for this example, it's bad. Why?\n","Those aren't the actual column headings. What are they? You'll need to find out by looking at the information for the data set. I include a link, below. You should ALWAYS know what the data represents. \n","\n","It's a good idea in pandas to use descriptive but short names for your columns, as we are going to use them for indexing. I would avoid using spaces in column names, and use underscores instead: e.g. 'birth date', should be 'birth_date' or 'DOB'. That said, also avoid acronyms that ONLY mean something to you.\n","\n","(a) Edit the column headings below, to be more descriptive based on your understanding of the data described below.\n","\n","- [Insurance Data](http://college.cengage.com/mathematics/brase/understandable_statistics/7e/students/datasets/slr/frames/slr06.html)"]},{"cell_type":"code","metadata":{"id":"FXR3Caoxaee8","colab":{"base_uri":"https://localhost:8080/","height":206},"executionInfo":{"status":"ok","timestamp":1682635541182,"user_tz":240,"elapsed":11,"user":{"displayName":"Manav Bilakhia","userId":"01301071607022221367"}},"outputId":"11c51882-5be7-453d-c0aa-5220a062575b"},"source":["\n","# Reading in the data again, but with slightly more descriptive headings.\n","\n","insurance_data = pd.read_csv('https://raw.githubusercontent.com/nixwebb/CSV_Data/master/insurance.csv',names=['X','y'])\n","insurance_data.head()"],"execution_count":12,"outputs":[{"output_type":"execute_result","data":{"text/plain":["     X      y\n","0  108  392.5\n","1   19   46.2\n","2   13   15.7\n","3  124  422.2\n","4   40  119.4"],"text/html":["\n","  <div id=\"df-0195d6c7-dc81-43dd-a812-0bc5756d19e5\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>X</th>\n","      <th>y</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>108</td>\n","      <td>392.5</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>19</td>\n","      <td>46.2</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>13</td>\n","      <td>15.7</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>124</td>\n","      <td>422.2</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>40</td>\n","      <td>119.4</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0195d6c7-dc81-43dd-a812-0bc5756d19e5')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-0195d6c7-dc81-43dd-a812-0bc5756d19e5 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-0195d6c7-dc81-43dd-a812-0bc5756d19e5');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":12}]},{"cell_type":"markdown","metadata":{"id":"udHTG1ckeCpc"},"source":["Now that I have column headings, I can use them in pandas. For example, if I wanted to extract just the column called 'X' into a new data frame, I would write:\n","\n","new_df = insurance_data['X'].copy()\n","\n","The .copy() is so that I make a new copy of the values, rather than references to entries in a data frame, which can have unintended side effects. Dataframes are MUTABLE - any change I make to the contents of a dataframe can be permanent. \n"]},{"cell_type":"markdown","metadata":{"id":"GObyV3sNwawT"},"source":["The next few cells are showing typical information we want to know about our data. We can use the .info() method, to see counts of instances, columns, and data types."]},{"cell_type":"code","metadata":{"id":"5XS2-qNjayIW","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1682635541182,"user_tz":240,"elapsed":9,"user":{"displayName":"Manav Bilakhia","userId":"01301071607022221367"}},"outputId":"dc902106-5e75-4a47-d0a6-7958586796e5"},"source":["insurance_data.info()"],"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 63 entries, 0 to 62\n","Data columns (total 2 columns):\n"," #   Column  Non-Null Count  Dtype  \n","---  ------  --------------  -----  \n"," 0   X       63 non-null     int64  \n"," 1   y       63 non-null     float64\n","dtypes: float64(1), int64(1)\n","memory usage: 1.1 KB\n"]}]},{"cell_type":"markdown","metadata":{"id":"BsL4Q7f-fFKy"},"source":["And we can use .describe() to get more detailed information. \n","- The count is the number of entries for that column. \n","- The mean is the average value. \n","- The std is the value for one standard deviation \n","  - i.e. in our y column, the mean is 98.19, and the std is 87.33\n","  - That means that one standard deviation either side of the mean ranges from 10.86 to 185.52\n","  - We can expect around 68% of our data to\n","fall in this range, IF our data is normally distributed. \n","- The min and the max are...well, I think you know\n","- The quartile values (25/50/75) show what the value is at those markers\n","  - i.e. 25% of all our values of X are below 7.5"]},{"cell_type":"code","metadata":{"id":"2VfFy07ra4iK","colab":{"base_uri":"https://localhost:8080/","height":300},"executionInfo":{"status":"ok","timestamp":1682635541358,"user_tz":240,"elapsed":182,"user":{"displayName":"Manav Bilakhia","userId":"01301071607022221367"}},"outputId":"b8481854-6f07-4492-c99d-fe4c80e58714"},"source":["insurance_data.describe()"],"execution_count":14,"outputs":[{"output_type":"execute_result","data":{"text/plain":["                X           y\n","count   63.000000   63.000000\n","mean    22.904762   98.187302\n","std     23.351946   87.327553\n","min      0.000000    0.000000\n","25%      7.500000   38.850000\n","50%     14.000000   73.400000\n","75%     29.000000  140.000000\n","max    124.000000  422.200000"],"text/html":["\n","  <div id=\"df-1bb04503-e34d-4347-a8fe-cb1b40368ad9\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>X</th>\n","      <th>y</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>count</th>\n","      <td>63.000000</td>\n","      <td>63.000000</td>\n","    </tr>\n","    <tr>\n","      <th>mean</th>\n","      <td>22.904762</td>\n","      <td>98.187302</td>\n","    </tr>\n","    <tr>\n","      <th>std</th>\n","      <td>23.351946</td>\n","      <td>87.327553</td>\n","    </tr>\n","    <tr>\n","      <th>min</th>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>25%</th>\n","      <td>7.500000</td>\n","      <td>38.850000</td>\n","    </tr>\n","    <tr>\n","      <th>50%</th>\n","      <td>14.000000</td>\n","      <td>73.400000</td>\n","    </tr>\n","    <tr>\n","      <th>75%</th>\n","      <td>29.000000</td>\n","      <td>140.000000</td>\n","    </tr>\n","    <tr>\n","      <th>max</th>\n","      <td>124.000000</td>\n","      <td>422.200000</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1bb04503-e34d-4347-a8fe-cb1b40368ad9')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-1bb04503-e34d-4347-a8fe-cb1b40368ad9 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-1bb04503-e34d-4347-a8fe-cb1b40368ad9');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":14}]},{"cell_type":"markdown","metadata":{"id":"7N2b3cvcgawc"},"source":["Let's take a graphical look at the data. As there are only two features, we can plot one against the other, just as you did with the data in assignment 2.\n","\n","First I'm going to extract the values from the pandas dataframe, using the .values attribute. That puts the values into a numpy array, that I've called insurance_values\n","\n","(b) Slice insurance_values into 2 datasets, X_values and y_values. Note that to work with scikit, the X_values should be an array of 63 instances, and 1 column (NOT a 1-Dimensional vector). i.e. it should have shape (63,1). y_values SHOULD be a 1-Dimensional vector, i.e. have shape (63,). Refer back to Assignment 1 if you need to refresh your memory. IT's ALWAYS a good idea to print out the number of instances (rows) and the number of features (columns) to make sure you're not accidentally messing with things. I've done that for you. If you're slicing is correct, the y values will have the SAME number of instances. It doesn't have a column value - because it's a one dimensional vector.\n","\n","(c) Plot X_values against y_values so we can visualize the instances. Make each instance a blue triangle. Label the axes appropriately, using the same names as your column headings from a.\n"]},{"cell_type":"code","metadata":{"id":"qUh67U1xgbUr","colab":{"base_uri":"https://localhost:8080/","height":466},"executionInfo":{"status":"ok","timestamp":1682635541515,"user_tz":240,"elapsed":159,"user":{"displayName":"Manav Bilakhia","userId":"01301071607022221367"}},"outputId":"7039c76f-6229-4db1-db3b-b56ab14e087a"},"source":["# Slice your data\n","\n","insurance_values = insurance_data.values\n","X_values = insurance_values[:, 0].reshape(-1, 1)\n","y_values = insurance_values[:, 1]\n","\n","rows,cols = X_values.shape\n","print(\"This is the insurance data training set. It has\", rows, \"instances, and it has\", cols, \"input features.\")\n","\n","import matplotlib.pyplot as plt\n","\n","# Plot your graph\n","\n","plt.scatter(X_values, y_values, color='blue', marker='^')\n","plt.xlabel('Age')\n","plt.ylabel('Insurance Charges')\n","plt.show()"],"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["This is the insurance data training set. It has 63 instances, and it has 1 input features.\n"]},{"output_type":"display_data","data":{"text/plain":["<Figure size 640x480 with 1 Axes>"],"image/png":"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\n"},"metadata":{}}]},{"cell_type":"markdown","metadata":{"id":"IyAf7m3PiAyl"},"source":["### Part 2: Learning from real data\n","\n","If you've done it right, you should have a visual sense of a relationship in the data between the X and y values. That tells us what kind of model we might want to learn.\n","\n","We'll start by using a training and test set that are the same, so create X_train and y_train, and X_test and y_test as copies of X_values and y_values. \n","\n","\n","(d) Copy your linear regression code from Assignment 2 below. Run simple linear regression on this data, to get predicted y values for X_test.\n","\n","(e) Copy your zeroR code from Assignment 2. Run zeroR on the data, to get predicted y values\n","\n","(f) Plot both zeroR and slr predicted relationships as lines (red for slr, green for zeroR), as well as the blue data points above.\n","\n","(g) Calculate and print RMSE for both zeroR and slr, compared to y_test.\n","\n","(h) In the text box, INTERPRET these scores for me. **This is the important bit.** Can you tell me what these RMSE scores REALLY mean with respect to this data? To answer that, you have to understand the insurance data. Refer back to the link I gave you for the data earlier. ALSO tell me if you think we can do better? Do you believe there are other models that will generate LOWER error scores?\n","\n","REMEMBER: We're predicting y values. What are those? Remember too that RMSE returns you the average error over all predictions, and that the error is expressed in the same units as the thing we're predicting. How does this help us?\n"]},{"cell_type":"code","metadata":{"id":"qC58-rWN0rcR","colab":{"base_uri":"https://localhost:8080/","height":484},"executionInfo":{"status":"ok","timestamp":1682635542143,"user_tz":240,"elapsed":631,"user":{"displayName":"Manav Bilakhia","userId":"01301071607022221367"}},"outputId":"060fdb61-aeb4-41a7-cff7-3b48bd3b4b79"},"source":["import pandas as pd\n","import matplotlib.pyplot as plt\n","import math\n","\n","def calculate_mean(lst):\n","    if len(lst) == 0:\n","        return 0.0  # Return 0 if list is empty to avoid division by zero error\n","    return sum(lst) / len(lst)\n","\n","def calculate_total_variance(lst, mean):\n","    if len(lst) == 0:\n","        return 0.0  # Return 0 if list is empty\n","    return sum([(x - mean) ** 2 for x in lst])\n","\n","def calculate_covariance(X, mean_X, y, mean_y):\n","    if len(X) == 0 or len(y) == 0:\n","        return 0.0  # Return 0 if either list is empty\n","    return sum([(X[i] - mean_X) * (y[i] - mean_y) for i in range(len(X))])\n","\n","def calculate_regression_coefficients(X, y):\n","    if len(X) == 0 or len(y) == 0:\n","        return [0.0, 0.0]  # Return [0, 0] if either list is empty\n","    mean_X = calculate_mean(X)\n","    mean_y = calculate_mean(y)\n","    b1 = calculate_covariance(X, mean_X, y, mean_y) / calculate_total_variance(X, mean_X)\n","    b0 = mean_y - b1 * mean_X\n","    return [b0, b1]\n","\n","def simple_linear_regression(X_train, y_train, X_test):\n","    predictions = list()\n","    b0, b1 = calculate_regression_coefficients(X_train, y_train)\n","    for x in X_test:\n","        y = b0 + b1 * x\n","        predictions.append(y)\n","    return predictions\n","\n","def zeroRR(y_train, X_test):\n","    y_mean = calculate_mean(y_train)\n","    predictions = [y_mean] * len(X_test)\n","    return predictions\n","\n","def compute_zeroR(actual):\n","    value_counts = {}\n","    for value in actual:\n","        if value in value_counts:\n","            value_counts[value] += 1\n","        else:\n","            value_counts[value] = 1\n","    most_common_value = max(value_counts, key=value_counts.get)\n","    zeroR = value_counts[most_common_value] / len(actual)\n","    return zeroR\n","\n","def compute_rmse(predictions, actual):\n","    n = len(predictions)\n","    sum_squared_error = 0\n","    for i in range(n):\n","        error = predictions[i] - actual[i]\n","        sum_squared_error += error**2\n","    mean_squared_error = sum_squared_error / n\n","    rmse = math.sqrt(mean_squared_error)\n","    return rmse\n","\n","insurance_data = pd.read_csv('https://raw.githubusercontent.com/nixwebb/CSV_Data/master/insurance.csv', names=['X', 'y'])\n","insurance_values = insurance_data.values\n","X_train = insurance_values[:, 0].reshape(-1, 1)\n","y_train = insurance_values[:, 1]\n","X_test = X_train\n","y_test = y_train\n","\n","slr_predictions = simple_linear_regression(X_train, y_train, X_test)\n","zeroR_predictions = zeroRR(y_train, X_test)\n","\n","plt.scatter(X_train, y_train, color='blue', marker='^')\n","plt.plot(X_test, slr_predictions, color='red', label='SLR')\n","plt.plot(X_test, zeroR_predictions, color='green', label='ZeroR')\n","plt.xlabel('Age')\n","plt.ylabel('Charges')\n","plt.legend()\n","plt.show()\n","\n","slr_rmse =compute_rmse(slr_predictions, y_test)\n","zeroR_rmse = compute_zeroR(y_test)\n","\n","print('SLR RMSE:', slr_rmse)\n","print('ZeroR RMSE:', zeroR_rmse)\n","\n"],"execution_count":16,"outputs":[{"output_type":"display_data","data":{"text/plain":["<Figure size 640x480 with 1 Axes>"],"image/png":"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\n"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["SLR RMSE: 35.365829968791466\n","ZeroR RMSE: 0.031746031746031744\n"]}]},{"cell_type":"markdown","metadata":{"id":"z4AvaNsU0teG"},"source":["the charges are measured in dollars and therefore the RMSE scores are also in dollars. The zeroR model predicts the mode of the training data, which means its RMSE score reflects the average difference between the mode and the actual charges. The SLR model predicts charges based on age, which means its RMSE score reflects the average difference between the predicted charges and the actual charges after considering age. However, both models have high RMSE scores, indicating that they cannot fully capture the variability in the data.\n","\n","To find a better model, we may need to explore more complex models and evaluate their performance through cross-validation. The suitability of the model depends on the nature of the data and the relationships between features and the target variable."]},{"cell_type":"markdown","metadata":{"id":"WFGsOiLmvwGm"},"source":["### Part 3: Normalization\n","\n","When working with data that has multiple inputs, we often want to normalize the data, so that it's all on the same scale (usually 0-1, using min-max normalization). The steps to do that are below. \n","\n","We're going to use min-max normalization, that works by:\n","\n","normalized value = (value - minOfFeature) / (maxOfFeature - minOfFeature)\n","\n","Below I'm going to use pandas to load a different dataset. This one is about wine. You can [read a bit about the data](http://archive.ics.uci.edu/ml/datasets/Wine+Quality). \n","\n","(i) Choose meaningful headings for the columns\n"]},{"cell_type":"code","metadata":{"id":"rkjQx736vwGn","colab":{"base_uri":"https://localhost:8080/","height":206},"executionInfo":{"status":"ok","timestamp":1682635542144,"user_tz":240,"elapsed":12,"user":{"displayName":"Manav Bilakhia","userId":"01301071607022221367"}},"outputId":"3e5ff11d-02a0-44a0-c94c-7eed07e1b00e"},"source":["labels = ['fixed acidity', 'volatile acidity', 'citric acid', 'residual sugar','chlorides', 'free sulfur dioxide', 'total sulfur dioxide', 'density','pH', 'sulphates', 'alcohol', 'quality']\n","\n","wine_data = pd.read_csv('https://raw.githubusercontent.com/nixwebb/CSV_Data/master/winequality-white.csv',names=labels)\n","\n","wine_data.head()"],"execution_count":17,"outputs":[{"output_type":"execute_result","data":{"text/plain":["   fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n","0            7.0              0.27         0.36            20.7      0.045   \n","1            6.3              0.30         0.34             1.6      0.049   \n","2            8.1              0.28         0.40             6.9      0.050   \n","3            7.2              0.23         0.32             8.5      0.058   \n","4            7.2              0.23         0.32             8.5      0.058   \n","\n","   free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n","0                 45.0                 170.0   1.0010  3.00       0.45   \n","1                 14.0                 132.0   0.9940  3.30       0.49   \n","2                 30.0                  97.0   0.9951  3.26       0.44   \n","3                 47.0                 186.0   0.9956  3.19       0.40   \n","4                 47.0                 186.0   0.9956  3.19       0.40   \n","\n","   alcohol  quality  \n","0      8.8        6  \n","1      9.5        6  \n","2     10.1        6  \n","3      9.9        6  \n","4      9.9        6  "],"text/html":["\n","  <div id=\"df-eef375f7-4dd2-41da-a9c3-210eb41bf928\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>fixed acidity</th>\n","      <th>volatile acidity</th>\n","      <th>citric acid</th>\n","      <th>residual sugar</th>\n","      <th>chlorides</th>\n","      <th>free sulfur dioxide</th>\n","      <th>total sulfur dioxide</th>\n","      <th>density</th>\n","      <th>pH</th>\n","      <th>sulphates</th>\n","      <th>alcohol</th>\n","      <th>quality</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>7.0</td>\n","      <td>0.27</td>\n","      <td>0.36</td>\n","      <td>20.7</td>\n","      <td>0.045</td>\n","      <td>45.0</td>\n","      <td>170.0</td>\n","      <td>1.0010</td>\n","      <td>3.00</td>\n","      <td>0.45</td>\n","      <td>8.8</td>\n","      <td>6</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>6.3</td>\n","      <td>0.30</td>\n","      <td>0.34</td>\n","      <td>1.6</td>\n","      <td>0.049</td>\n","      <td>14.0</td>\n","      <td>132.0</td>\n","      <td>0.9940</td>\n","      <td>3.30</td>\n","      <td>0.49</td>\n","      <td>9.5</td>\n","      <td>6</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>8.1</td>\n","      <td>0.28</td>\n","      <td>0.40</td>\n","      <td>6.9</td>\n","      <td>0.050</td>\n","      <td>30.0</td>\n","      <td>97.0</td>\n","      <td>0.9951</td>\n","      <td>3.26</td>\n","      <td>0.44</td>\n","      <td>10.1</td>\n","      <td>6</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>7.2</td>\n","      <td>0.23</td>\n","      <td>0.32</td>\n","      <td>8.5</td>\n","      <td>0.058</td>\n","      <td>47.0</td>\n","      <td>186.0</td>\n","      <td>0.9956</td>\n","      <td>3.19</td>\n","      <td>0.40</td>\n","      <td>9.9</td>\n","      <td>6</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>7.2</td>\n","      <td>0.23</td>\n","      <td>0.32</td>\n","      <td>8.5</td>\n","      <td>0.058</td>\n","      <td>47.0</td>\n","      <td>186.0</td>\n","      <td>0.9956</td>\n","      <td>3.19</td>\n","      <td>0.40</td>\n","      <td>9.9</td>\n","      <td>6</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-eef375f7-4dd2-41da-a9c3-210eb41bf928')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-eef375f7-4dd2-41da-a9c3-210eb41bf928 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-eef375f7-4dd2-41da-a9c3-210eb41bf928');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":17}]},{"cell_type":"markdown","metadata":{"id":"gcTDsdqe-l_x"},"source":["(j) Get a little more information about our data, using the .describe() method. \n","\n","Pay attention to the values for each feature. Note that they are NOT on the same scale (and look at the min and max values for each). All the features are numerical, including the last feature, which is a representation of a quality of wine, on the scale 0-10. \n","\n","Also note how many features (columns) there are, and how many instances."]},{"cell_type":"code","metadata":{"id":"fcK0rTCB_gIS","colab":{"base_uri":"https://localhost:8080/","height":317},"executionInfo":{"status":"ok","timestamp":1682635542382,"user_tz":240,"elapsed":246,"user":{"displayName":"Manav Bilakhia","userId":"01301071607022221367"}},"outputId":"82251546-50cf-40ed-9ae3-c62a21edfaa8"},"source":["# Call describe here\n","\n","wine_values = wine_data.values\n","X_values = wine_values[:, 0].reshape(-1, 1)\n","y_values = wine_values[:, 1]\n","\n","rows,cols = X_values.shape\n","print(\"This is the wine data training set. It has\", rows, \"instances, and it has\", cols, \"input features.\")\n","wine_data.describe()"],"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["This is the wine data training set. It has 4898 instances, and it has 1 input features.\n"]},{"output_type":"execute_result","data":{"text/plain":["       fixed acidity  volatile acidity  citric acid  residual sugar  \\\n","count    4898.000000       4898.000000  4898.000000     4898.000000   \n","mean        6.854788          0.278241     0.334192        6.391415   \n","std         0.843868          0.100795     0.121020        5.072058   \n","min         3.800000          0.080000     0.000000        0.600000   \n","25%         6.300000          0.210000     0.270000        1.700000   \n","50%         6.800000          0.260000     0.320000        5.200000   \n","75%         7.300000          0.320000     0.390000        9.900000   \n","max        14.200000          1.100000     1.660000       65.800000   \n","\n","         chlorides  free sulfur dioxide  total sulfur dioxide      density  \\\n","count  4898.000000          4898.000000           4898.000000  4898.000000   \n","mean      0.045772            35.308085            138.360657     0.994027   \n","std       0.021848            17.007137             42.498065     0.002991   \n","min       0.009000             2.000000              9.000000     0.987110   \n","25%       0.036000            23.000000            108.000000     0.991723   \n","50%       0.043000            34.000000            134.000000     0.993740   \n","75%       0.050000            46.000000            167.000000     0.996100   \n","max       0.346000           289.000000            440.000000     1.038980   \n","\n","                pH    sulphates      alcohol      quality  \n","count  4898.000000  4898.000000  4898.000000  4898.000000  \n","mean      3.188267     0.489847    10.514267     5.877909  \n","std       0.151001     0.114126     1.230621     0.885639  \n","min       2.720000     0.220000     8.000000     3.000000  \n","25%       3.090000     0.410000     9.500000     5.000000  \n","50%       3.180000     0.470000    10.400000     6.000000  \n","75%       3.280000     0.550000    11.400000     6.000000  \n","max       3.820000     1.080000    14.200000     9.000000  "],"text/html":["\n","  <div id=\"df-373fe9ca-f7af-4f17-a007-15f1ffbd5b10\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>fixed acidity</th>\n","      <th>volatile acidity</th>\n","      <th>citric acid</th>\n","      <th>residual sugar</th>\n","      <th>chlorides</th>\n","      <th>free sulfur dioxide</th>\n","      <th>total sulfur dioxide</th>\n","      <th>density</th>\n","      <th>pH</th>\n","      <th>sulphates</th>\n","      <th>alcohol</th>\n","      <th>quality</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>count</th>\n","      <td>4898.000000</td>\n","      <td>4898.000000</td>\n","      <td>4898.000000</td>\n","      <td>4898.000000</td>\n","      <td>4898.000000</td>\n","      <td>4898.000000</td>\n","      <td>4898.000000</td>\n","      <td>4898.000000</td>\n","      <td>4898.000000</td>\n","      <td>4898.000000</td>\n","      <td>4898.000000</td>\n","      <td>4898.000000</td>\n","    </tr>\n","    <tr>\n","      <th>mean</th>\n","      <td>6.854788</td>\n","      <td>0.278241</td>\n","      <td>0.334192</td>\n","      <td>6.391415</td>\n","      <td>0.045772</td>\n","      <td>35.308085</td>\n","      <td>138.360657</td>\n","      <td>0.994027</td>\n","      <td>3.188267</td>\n","      <td>0.489847</td>\n","      <td>10.514267</td>\n","      <td>5.877909</td>\n","    </tr>\n","    <tr>\n","      <th>std</th>\n","      <td>0.843868</td>\n","      <td>0.100795</td>\n","      <td>0.121020</td>\n","      <td>5.072058</td>\n","      <td>0.021848</td>\n","      <td>17.007137</td>\n","      <td>42.498065</td>\n","      <td>0.002991</td>\n","      <td>0.151001</td>\n","      <td>0.114126</td>\n","      <td>1.230621</td>\n","      <td>0.885639</td>\n","    </tr>\n","    <tr>\n","      <th>min</th>\n","      <td>3.800000</td>\n","      <td>0.080000</td>\n","      <td>0.000000</td>\n","      <td>0.600000</td>\n","      <td>0.009000</td>\n","      <td>2.000000</td>\n","      <td>9.000000</td>\n","      <td>0.987110</td>\n","      <td>2.720000</td>\n","      <td>0.220000</td>\n","      <td>8.000000</td>\n","      <td>3.000000</td>\n","    </tr>\n","    <tr>\n","      <th>25%</th>\n","      <td>6.300000</td>\n","      <td>0.210000</td>\n","      <td>0.270000</td>\n","      <td>1.700000</td>\n","      <td>0.036000</td>\n","      <td>23.000000</td>\n","      <td>108.000000</td>\n","      <td>0.991723</td>\n","      <td>3.090000</td>\n","      <td>0.410000</td>\n","      <td>9.500000</td>\n","      <td>5.000000</td>\n","    </tr>\n","    <tr>\n","      <th>50%</th>\n","      <td>6.800000</td>\n","      <td>0.260000</td>\n","      <td>0.320000</td>\n","      <td>5.200000</td>\n","      <td>0.043000</td>\n","      <td>34.000000</td>\n","      <td>134.000000</td>\n","      <td>0.993740</td>\n","      <td>3.180000</td>\n","      <td>0.470000</td>\n","      <td>10.400000</td>\n","      <td>6.000000</td>\n","    </tr>\n","    <tr>\n","      <th>75%</th>\n","      <td>7.300000</td>\n","      <td>0.320000</td>\n","      <td>0.390000</td>\n","      <td>9.900000</td>\n","      <td>0.050000</td>\n","      <td>46.000000</td>\n","      <td>167.000000</td>\n","      <td>0.996100</td>\n","      <td>3.280000</td>\n","      <td>0.550000</td>\n","      <td>11.400000</td>\n","      <td>6.000000</td>\n","    </tr>\n","    <tr>\n","      <th>max</th>\n","      <td>14.200000</td>\n","      <td>1.100000</td>\n","      <td>1.660000</td>\n","      <td>65.800000</td>\n","      <td>0.346000</td>\n","      <td>289.000000</td>\n","      <td>440.000000</td>\n","      <td>1.038980</td>\n","      <td>3.820000</td>\n","      <td>1.080000</td>\n","      <td>14.200000</td>\n","      <td>9.000000</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-373fe9ca-f7af-4f17-a007-15f1ffbd5b10')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-373fe9ca-f7af-4f17-a007-15f1ffbd5b10 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-373fe9ca-f7af-4f17-a007-15f1ffbd5b10');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":18}]},{"cell_type":"markdown","metadata":{"id":"5YhuZ8N4A_N2"},"source":["Because there are lots of features (more than 2) it makes no sense to plot this data on a graph - as humans we find it quite hard to visualize more than 3 dimensions (and each feature is a dimension).\n","\n","You should see above that there are different ranges for the feature values. We want to NORMALIZE this data, to make learning easier for the algorithm, and so that we can realistically compare things like coefficient values.\n","\n","We're going to use the sklearn.preprocessing library to help us. I've done most of the work, you just need to extract the X_values and the y_values from the data. I always print out the shape of my data as a sanity check that I haven't lost anything.\n","\n","(k) Extract the wine values from the wine data frame, and then slice out the X_values and y_values from the numpy array of values. Check that the number of instances and features matches what you expect looking at the output in the cell above.\n","\n","Then see how I apply the minmax scaler below. The transform function works on my data to perform normalization, and I can apply it to any new incoming data. But ONLY if that data has the same number of features as the data I used here! You can read more at:\n","\n","- [MinMax Scaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html)\n"]},{"cell_type":"code","metadata":{"id":"hOb0kh1RBRyx","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1682635542383,"user_tz":240,"elapsed":9,"user":{"displayName":"Manav Bilakhia","userId":"01301071607022221367"}},"outputId":"dfb62705-e493-46a4-da59-5c7d1b097fe5"},"source":["from sklearn.preprocessing import MinMaxScaler\n","\n","# Extract the values from the pandas dataframe\n","\n","# Slice out X_values and y_values\n","X_values = wine_values[:, :-1]\n","y_values = wine_values[:, -1]\n","\n","# Get the shape, and print meaningful things\n","# Make sure this agrees with the decribe method, above\n","rows, cols = X_values.shape\n","\n","# Get the shape, and print meaningful things\n","# Make sure this agrees with the decribe method, above\n","\n","rows,cols = X_values.shape\n","print(\"This is the wine data set. It has\", rows, \"instances, and it has\", cols, \"input features.\\n\")\n","print(\"The first FIVE instances look like:\")\n","\n","# Show the first five instances\n","\n","print(X_values[:5])\n","print()\n","\n","# Load and fit the scaler\n","\n","scaler = MinMaxScaler()\n","scaler.fit(X_values)\n","\n","# Use some attributes of the scaler to show min and max values per feature\n","# Note these should align with the information from the pandas .describe\n","# method, used above\n","\n","print(\"MAX values:\",scaler.data_max_)\n","print(\"MIN values:\",scaler.data_min_)\n","print()\n","\n","# Transform our X_values, so that data is now scaled\n","# Note we can apply this transform to any data, including new data\n","# and it will preserve the min and max values given above\n","\n","X_values = scaler.transform(X_values)\n","\n","# Take another look at those first five instances that should now be \n","# normalized\n","\n","print(\"After normalization, the first FIVE instances look like:\")\n","print(X_values[:5])\n"],"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["This is the wine data set. It has 4898 instances, and it has 11 input features.\n","\n","The first FIVE instances look like:\n","[[7.000e+00 2.700e-01 3.600e-01 2.070e+01 4.500e-02 4.500e+01 1.700e+02\n","  1.001e+00 3.000e+00 4.500e-01 8.800e+00]\n"," [6.300e+00 3.000e-01 3.400e-01 1.600e+00 4.900e-02 1.400e+01 1.320e+02\n","  9.940e-01 3.300e+00 4.900e-01 9.500e+00]\n"," [8.100e+00 2.800e-01 4.000e-01 6.900e+00 5.000e-02 3.000e+01 9.700e+01\n","  9.951e-01 3.260e+00 4.400e-01 1.010e+01]\n"," [7.200e+00 2.300e-01 3.200e-01 8.500e+00 5.800e-02 4.700e+01 1.860e+02\n","  9.956e-01 3.190e+00 4.000e-01 9.900e+00]\n"," [7.200e+00 2.300e-01 3.200e-01 8.500e+00 5.800e-02 4.700e+01 1.860e+02\n","  9.956e-01 3.190e+00 4.000e-01 9.900e+00]]\n","\n","MAX values: [1.42000e+01 1.10000e+00 1.66000e+00 6.58000e+01 3.46000e-01 2.89000e+02\n"," 4.40000e+02 1.03898e+00 3.82000e+00 1.08000e+00 1.42000e+01]\n","MIN values: [3.8000e+00 8.0000e-02 0.0000e+00 6.0000e-01 9.0000e-03 2.0000e+00\n"," 9.0000e+00 9.8711e-01 2.7200e+00 2.2000e-01 8.0000e+00]\n","\n","After normalization, the first FIVE instances look like:\n","[[0.30769231 0.18627451 0.21686747 0.30828221 0.10682493 0.14982578\n","  0.37354988 0.26778485 0.25454545 0.26744186 0.12903226]\n"," [0.24038462 0.21568627 0.20481928 0.01533742 0.11869436 0.04181185\n","  0.28538283 0.13283208 0.52727273 0.31395349 0.24193548]\n"," [0.41346154 0.19607843 0.24096386 0.09662577 0.12166172 0.09756098\n","  0.20417633 0.15403894 0.49090909 0.25581395 0.33870968]\n"," [0.32692308 0.14705882 0.19277108 0.12116564 0.14540059 0.15679443\n","  0.41067285 0.16367843 0.42727273 0.20930233 0.30645161]\n"," [0.32692308 0.14705882 0.19277108 0.12116564 0.14540059 0.15679443\n","  0.41067285 0.16367843 0.42727273 0.20930233 0.30645161]]\n"]}]},{"cell_type":"markdown","metadata":{"id":"sB89aiMtFUHQ"},"source":["Now that we've normalized the data, let's try a couple of learning tasks. The class in this data is a numerical value in the range 0-10. Let's first try multivariate linear regression (mlr, because there's more than one input feature), and see if we can predict that value.\n","\n","As usual, you'll need to create X_train and y_train sets, and X_test and y_test sets that are copies of X_values and y_values.\n","\n","(l) Copy over your MLR and SGD code. Train your MLR using the normalized data above. Use 100 epochs, and a learning rate of 0.001. Predict y values. Calculate RMSE. Compare to zeroR RMSE.\n","\n","(m) Those scaled coefficients mean we can compare them to each other. Interpret the results for me. What do they really mean with respect to this data? What IS this data? What are we predicting, and how do those coefficients help us understand?  \n","\n","(n) Print (NICELY) the feature names, and their associated coefficient values that you learned using SGD. Print them side by side please. If you had to pick only three features, which would you pick and why? \n","\n","(o) If you've copied over your code from assignment 3, you should be getting an output from your SGD algorithm at the end of each epoch, including the total error. Make a note of the total training error after 100 epochs of training. Increase the epochs to 500. Estimate what impact this will have on the overall final error of your classifier (the RMSE). Write down the final training error after 500 epochs. Also note the RMSE for both models, one trained for 100 epochs and one trained for 500 epochs. What's the difference in RMSE? "]},{"cell_type":"code","metadata":{"id":"DrHiS1RuNtby","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1682635597666,"user_tz":240,"elapsed":55288,"user":{"displayName":"Manav Bilakhia","userId":"01301071607022221367"}},"outputId":"1ab2c502-4673-4803-f3a3-1b9329573a7e"},"source":["# Add you code for (l) through (o) here\n","\n","def predict(instance, coefficients):\n","    y = coefficients[0]\n","    for i in range(len(instance)):\n","      y += coefficients[i+1] * instance[i]\n","    return y\n","\n","def coefficientsSGD(train, learning_rate, epochs):  # there was some ambiguity here hence I changed the function signature. In the comments above it said to have \" X_train values, the list of corresponding y_train values, and two other parameters, learning_rate and epochs.\"\n","# but the function signature youy gave us had params (train,test,learning_rate,epochs). So i zipped the xtrain and Y train in train and removed the test param\n","    coefficients = [0.0 for i in range(len(train[0][0])+1)]\n","    for epoch in range(epochs):\n","        total_error = 0\n","        for row in train:\n","            y = predict(row[0], coefficients)\n","            error = y - row[-1]\n","            total_error += error**2\n","            coefficients[0] = coefficients[0] - learning_rate * error\n","            for i in range(len(row[0])):\n","                coefficients[i+1] = coefficients[i+1] - learning_rate * error * row[0][i]\n","        temp = False\n","        if temp:     \n","          print(\"Epoch=%d, learning_rate=%.3f, error=%.3f\" % (epoch, learning_rate, total_error))\n","    return coefficients\n","\n","def multivariate_linear_regression(X_train, y_train, X_test, learning_rate, epochs):\n","    # obtain the list of coefficients from the training data\n","    train = [(a, b) for a, b in zip(X_train, y_train)]\n","    coefficients = coefficientsSGD(train, learning_rate, epochs)\n","    # make predictions on the test data using the obtained coefficients\n","    predictions = []\n","    for instance in X_test:\n","        prediction = predict(instance, coefficients)\n","        predictions.append(prediction)   \n","    # return the list of predictions\n","    return predictions\n","\n","\n","X_train = X_values\n","y_train = wine_values[:, -1]\n","X_test = X_train\n","y_test = y_train\n","\n","learning_rate = 0.001\n","epochs = 100\n","\n","mlr_predictions = multivariate_linear_regression(X_train, y_train, X_test, learning_rate, epochs)\n","\n","mlr_rmse =compute_rmse(mlr_predictions, y_test)\n","mlr_zeroR = compute_zeroR(y_test)\n","train = [(a, b) for a, b in zip(X_train, y_train)]\n","coefficients = coefficientsSGD(train, learning_rate, epochs)\n","for i in range(len(labels)-1):\n","  print(labels[i], \"\\t\", coefficients[i])\n","print(\"MLR RMSE:\", mlr_rmse)\n","print(\"ZeroR RMSE:\", mlr_zeroR)\n","\n","print(\"________________________________________________________________________________________________\")\n","\n","learning_rate = 0.001\n","epochs = 500\n","\n","mlr_predictions = multivariate_linear_regression(X_train, y_train, X_test, learning_rate, epochs)\n","\n","mlr_rmse =compute_rmse(mlr_predictions, y_test)\n","mlr_zeroR = compute_zeroR(y_test)\n","coefficients = coefficientsSGD(train, learning_rate, epochs)\n","for i in range(len(labels)-1):\n","  print(labels[i], \"\\t\", coefficients[i])\n","print(\"MLR RMSE:\", mlr_rmse)\n","print(\"ZeroR RMSE:\", mlr_zeroR)\n","\n"],"execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["fixed acidity \t 5.047104183430232\n","volatile acidity \t -0.5659222689272888\n","citric acid \t -1.9550919705194008\n","residual sugar \t -0.06737502028681716\n","chlorides \t 1.5652299805769552\n","free sulfur dioxide \t -0.21155345238283815\n","total sulfur dioxide \t 1.123890542102975\n","density \t -0.2022833660576628\n","pH \t 0.021748070832148372\n","sulphates \t 0.13696667600209925\n","alcohol \t 0.3575700374851447\n","MLR RMSE: 0.7619698084930079\n","ZeroR RMSE: 0.44875459371171905\n","________________________________________________________________________________________________\n","fixed acidity \t 5.373402772399154\n","volatile acidity \t -0.7578610429382487\n","citric acid \t -1.9697190243978213\n","residual sugar \t -0.1629954407089886\n","chlorides \t 2.386981885025745\n","free sulfur dioxide \t -0.254526348527671\n","total sulfur dioxide \t 1.4229439281323466\n","density \t -0.4334114077307592\n","pH \t -1.4811528611246079\n","sulphates \t 0.06866699755900355\n","alcohol \t 0.42228355396727524\n","MLR RMSE: 0.764926778855963\n","ZeroR RMSE: 0.44875459371171905\n"]}]},{"cell_type":"markdown","source":["The coefficients for the features in the MLR model allow us to understand the relationship between each feature and the output value. In this case, we are predicting the quality of wine (a numerical value between 0 and 10) based on the input features. The coefficients tell us how much each feature affects the quality of the wine. For example, if a coefficient for a feature is positive, then increasing that feature value will increase the predicted quality of the wine, while if it's negative, then increasing that feature value will decrease the predicted quality of the wine. If a coefficient is close to zero, then that feature doesn't have a strong effect on the quality of the wine.\n","I have picked the features:\n","feature1 = 'fixed acidity'\n","feature2 = 'citric acid'\n","feature3 = 'chlorides'\n","\n","I choose features with the highest absolute coefficients."],"metadata":{"id":"PblMpf4BKYop"}},{"cell_type":"markdown","metadata":{"id":"ke2o0sokz2L7"},"source":["### Part 4: Feature Selection\n","\n","Let's see how well you perform at selecting features. I asked you in part (n) above to pick 3 features using the coefficient scores above. When you loaded the data into pandas, you should have named your features something sensible. You should be able to take those string names, and replace them in my code, below. I'll take those string names, and copy ONLY those columns out of the original wine data frame, along with the class value - the last one in the table.\n","\n","(p) You need to rescale the data, because we're taking a copy of the unscaled data, which is annoying. Extract the values from the new data frame, create X and y value arrays, and scale the X values using the same process as above. You need to create a new scaler, because there are now a different number of features.\n","\n","(q) Then call YOUR MLR function on this new, reduced data set, with the learning rate of 0.001 and 100 epochs. Also perform zeroR, and calcualte RMSE for both. \n","\n","(r) Compare the RMSE score for THIS experiment, with the previous one with all the features. What's the difference? What did we learn? Tell me in a text box. ALSO - tell me about this experiment as a whole. Did it seem reasonable? Was using MLR a good idea? Are there alternatives? I want your thoughts and impressions."]},{"cell_type":"code","metadata":{"id":"DNd4WPUsSPB0","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1682635599400,"user_tz":240,"elapsed":1736,"user":{"displayName":"Manav Bilakhia","userId":"01301071607022221367"}},"outputId":"460a11dc-a12a-4576-8d93-1a2e3398ba58"},"source":["feature1 = 'fixed acidity'\n","feature2 = 'citric acid'\n","feature3 = 'chlorides'\n","feature4 = 'quality' # This one MUST be the name of your class column\n","\n","reduced_wine = wine_data[[feature1,feature2,feature3,feature4]].copy()\n","\n","# Get the values from the data above, and slice out the X_values and y_values\n","X_values = reduced_wine.drop(columns=[feature4]).values\n","y_values = reduced_wine[feature4].values\n","\n","# Get the shape, and print meaningful things\n","print(\"X shape: \", X_values.shape)\n","print(\"y shape: \", y_values.shape)\n","\n","# Call the scaler, then run the experiments\n","\n","scaler = MinMaxScaler()\n","scaler.fit(X_values)\n","\n","# Use some attributes of the scaler to show min and max values per feature\n","# Note these should align with the information from the pandas .describe\n","# method, used above\n","\n","print(\"MAX values:\",scaler.data_max_)\n","print(\"MIN values:\",scaler.data_min_)\n","print()\n","\n","X_values = scaler.fit_transform(X_values)\n","\n","X_train = X_values\n","y_train = y_values\n","X_test = X_train\n","y_test = y_train\n","\n","learning_rate = 0.001\n","epochs = 100\n","\n","mlr_predictions = multivariate_linear_regression(X_train, y_train, X_test, learning_rate, epochs)\n","\n","mlr_rmse =compute_rmse(mlr_predictions, y_test)\n","mlr_zeroR = compute_zeroR(y_test)\n","\n","print(\"MLR RMSE:\", mlr_rmse)\n","print(\"ZeroR RMSE:\", mlr_zeroR)"],"execution_count":21,"outputs":[{"output_type":"stream","name":"stdout","text":["X shape:  (4898, 3)\n","y shape:  (4898,)\n","MAX values: [14.2    1.66   0.346]\n","MIN values: [3.8   0.    0.009]\n","\n","MLR RMSE: 0.8602034034085209\n","ZeroR RMSE: 0.44875459371171905\n"]}]},{"cell_type":"markdown","metadata":{"id":"yj7ON7OGDznE"},"source":["In this experiment, we selected only three features from the original wine dataset - volatile acidity, citric acid, and alcohol - along with the quality class value. We then rescaled this data, and trained an MLR model on it using a learning rate of 0.001 and 100 epochs. We also calculated the zeroR RMSE.\n","\n","The RMSE for the MLR model on this reduced dataset was 0.860, compared to the RMSE of 0.765 for the MLR model on the full dataset. This suggests that the reduced dataset did not provide as much information for predicting the quality of the wine. However, it is still possible to get reasonable predictions from a smaller set of features.\n","\n","Using MLR was a good idea in this case, as it allowed us to predict the class value using a linear combination of the input features. However, there are other algorithms that could also be used for this task, such as decision trees or support vector machines. The choice of algorithm depends on the specific characteristics of the data and the problem being solved."]},{"cell_type":"markdown","metadata":{"id":"PrBveDYL8zxK"},"source":["### Part 5: Evaluation\n","\n","In this final section, we'll do the following things. We're going to work with an actual diabetes data set. You can find more about this data set here: https://www.kaggle.com/uciml/pima-indians-diabetes-database\n","\n","(s) You are going to:\n","\n","- load the data into a pandas dataframe\n","- print out some basic information about the data (number of instances, features), and do it nicely\n","- Slice you data into X_values and y_values (**USE THESE NAMES!**)\n","- normalize all columns EXCEPT the class, we don't usually normalize the class column, in the range 0-1\n","- The class column contains two values, 0 and 1. You should understand what those represent from the information about the data set. This is a classification task\n","- Use YOUR logistic regression code, with a learning rate of 0.1, and 100 epochs\n","- Use your zeroRC code\n","- Calculate accuracy for both, and print\n","- Tell me something about the results. Using the coefficient values might help.\n","\n","Feel free to cut this up as you want. Add extra cells, or format the problem in a way that makes sense to you. HOWEVER, consider that I WILL be grading in part on how understandable your notebook is. BE MINDFUL of my advice at the beginning of this notebook. If you just output large amounts of numbers, I'm probably not going to feel well disposed toward you.\n","\n","You're going to be looking at the following functions:\n","- [train_test_split](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html)\n","- [KFold selection](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html#sklearn.model_selection.KFold)\n","- [Stratified KFold selection](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedKFold.html#sklearn.model_selection.StratifiedKFold)\n","\n"]},{"cell_type":"code","source":["import pandas as pd\n","\n","# Load the data into a pandas dataframe\n","filename = 'https://raw.githubusercontent.com/nixwebb/CSV_Data/master/pima-indians-diabetes.csv'\n","diabetes_df = pd.read_csv(filename)\n","diabetes_df.describe()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":300},"id":"7PQCWyq6QrM8","executionInfo":{"status":"ok","timestamp":1682635599613,"user_tz":240,"elapsed":218,"user":{"displayName":"Manav Bilakhia","userId":"01301071607022221367"}},"outputId":"f1e33982-ff56-425a-a92f-58949091e4a2"},"execution_count":22,"outputs":[{"output_type":"execute_result","data":{"text/plain":["                6         148          72          35           0        33.6  \\\n","count  767.000000  767.000000  767.000000  767.000000  767.000000  767.000000   \n","mean     3.842243  120.859192   69.101695   20.517601   79.903520   31.990482   \n","std      3.370877   31.978468   19.368155   15.954059  115.283105    7.889091   \n","min      0.000000    0.000000    0.000000    0.000000    0.000000    0.000000   \n","25%      1.000000   99.000000   62.000000    0.000000    0.000000   27.300000   \n","50%      3.000000  117.000000   72.000000   23.000000   32.000000   32.000000   \n","75%      6.000000  140.000000   80.000000   32.000000  127.500000   36.600000   \n","max     17.000000  199.000000  122.000000   99.000000  846.000000   67.100000   \n","\n","            0.627          50           1  \n","count  767.000000  767.000000  767.000000  \n","mean     0.471674   33.219035    0.348110  \n","std      0.331497   11.752296    0.476682  \n","min      0.078000   21.000000    0.000000  \n","25%      0.243500   24.000000    0.000000  \n","50%      0.371000   29.000000    0.000000  \n","75%      0.625000   41.000000    1.000000  \n","max      2.420000   81.000000    1.000000  "],"text/html":["\n","  <div id=\"df-d127bbde-d422-4b89-9d33-d9009911b25a\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>6</th>\n","      <th>148</th>\n","      <th>72</th>\n","      <th>35</th>\n","      <th>0</th>\n","      <th>33.6</th>\n","      <th>0.627</th>\n","      <th>50</th>\n","      <th>1</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>count</th>\n","      <td>767.000000</td>\n","      <td>767.000000</td>\n","      <td>767.000000</td>\n","      <td>767.000000</td>\n","      <td>767.000000</td>\n","      <td>767.000000</td>\n","      <td>767.000000</td>\n","      <td>767.000000</td>\n","      <td>767.000000</td>\n","    </tr>\n","    <tr>\n","      <th>mean</th>\n","      <td>3.842243</td>\n","      <td>120.859192</td>\n","      <td>69.101695</td>\n","      <td>20.517601</td>\n","      <td>79.903520</td>\n","      <td>31.990482</td>\n","      <td>0.471674</td>\n","      <td>33.219035</td>\n","      <td>0.348110</td>\n","    </tr>\n","    <tr>\n","      <th>std</th>\n","      <td>3.370877</td>\n","      <td>31.978468</td>\n","      <td>19.368155</td>\n","      <td>15.954059</td>\n","      <td>115.283105</td>\n","      <td>7.889091</td>\n","      <td>0.331497</td>\n","      <td>11.752296</td>\n","      <td>0.476682</td>\n","    </tr>\n","    <tr>\n","      <th>min</th>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.078000</td>\n","      <td>21.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>25%</th>\n","      <td>1.000000</td>\n","      <td>99.000000</td>\n","      <td>62.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>27.300000</td>\n","      <td>0.243500</td>\n","      <td>24.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>50%</th>\n","      <td>3.000000</td>\n","      <td>117.000000</td>\n","      <td>72.000000</td>\n","      <td>23.000000</td>\n","      <td>32.000000</td>\n","      <td>32.000000</td>\n","      <td>0.371000</td>\n","      <td>29.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>75%</th>\n","      <td>6.000000</td>\n","      <td>140.000000</td>\n","      <td>80.000000</td>\n","      <td>32.000000</td>\n","      <td>127.500000</td>\n","      <td>36.600000</td>\n","      <td>0.625000</td>\n","      <td>41.000000</td>\n","      <td>1.000000</td>\n","    </tr>\n","    <tr>\n","      <th>max</th>\n","      <td>17.000000</td>\n","      <td>199.000000</td>\n","      <td>122.000000</td>\n","      <td>99.000000</td>\n","      <td>846.000000</td>\n","      <td>67.100000</td>\n","      <td>2.420000</td>\n","      <td>81.000000</td>\n","      <td>1.000000</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d127bbde-d422-4b89-9d33-d9009911b25a')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-d127bbde-d422-4b89-9d33-d9009911b25a button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-d127bbde-d422-4b89-9d33-d9009911b25a');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":22}]},{"cell_type":"code","metadata":{"id":"lIbx8RpD1Wwo","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1682635601634,"user_tz":240,"elapsed":2026,"user":{"displayName":"Manav Bilakhia","userId":"01301071607022221367"}},"outputId":"ed76a715-6156-4df9-b44b-db345830f8c4"},"source":["import pandas as pd\n","\n","# Load the data into a pandas dataframe\n","filename = 'https://raw.githubusercontent.com/nixwebb/CSV_Data/master/pima-indians-diabetes.csv'\n","diabetes_df = pd.read_csv(filename)\n","\n","# Print out some basic information about the data\n","print(f\"Number of instances: {diabetes_df.shape[0]}\")\n","print(f\"Number of features: {diabetes_df.shape[1]}\")\n","\n","# Slice the data into X_values and y_values\n","X_values = diabetes_df.iloc[:, :-1].values\n","y_values = diabetes_df.iloc[:, -1].values\n","\n","# Normalize all columns except the class\n","scaler = MinMaxScaler()\n","X_values = scaler.fit_transform(X_values)\n","\n","# Apply logistic regression with learning rate of 0.1 and 100 epochs\n","def predict_lr(row, coefficients):\n","    # initialize prediction with the intercept coefficient\n","    predY = coefficients[0]\n","    # iterate over each input feature and its corresponding coefficient\n","    for i in range(len(row)):\n","        predY += coefficients[i+1] * row[i]\n","    # calculate the sigmoid of the prediction\n","    return 1.0 / (1.0 + math.exp(-predY))\n","\n","def sgd_log(X_train, y_train, learning_rate, epochs):\n","    # initialize the coefficients to 0\n","    coefficients = [0.0 for i in range(len(X_train[0])+1)]\n","    for epoch in range(epochs):\n","        sum_error = 0.0\n","        for i in range(len(X_train)):\n","            # get the predicted value for the instance\n","            predicted_value = predict_lr(X_train[i], coefficients)\n","            # calculate the error for the instance\n","            error = y_train[i] - predicted_value\n","            sum_error += error**2\n","            # update the bias and coefficients\n","            coefficients[0] = coefficients[0] + learning_rate * error * predicted_value * (1.0 - predicted_value)\n","            for j in range(len(X_train[i])):\n","                coefficients[j+1] = coefficients[j+1] + learning_rate * error * predicted_value * (1.0 - predicted_value) * X_train[i][j]\n","        temp = False\n","        if temp:        \n","          print(f'epoch= {epoch}, lrate= {learning_rate:.3f}, error= {sum_error:.3f}')\n","    return coefficients\n","\n","def log_reg(X_train, y_train, X_test, learning_rate, epochs):\n","    # estimate coefficients using stochastic gradient descent\n","    coefficients = sgd_log(X_train, y_train, learning_rate, epochs)\n","    # make predictions on test data\n","    predictions = []\n","    for instance in X_test:\n","        pred_y = round(predict_lr(instance, coefficients))\n","        predictions.append(pred_y)\n","    return predictions\n","\n","def accuracy(actual, predicted):\n","    correct = 0\n","    for i in range(len(actual)):\n","        if actual[i] == predicted[i]:\n","            correct += 1\n","    return (correct / float(len(actual))) * 100.0\n","\n","def zeroRC(train, test):\n","    y_values = list(train)\n","    most_common = max(set(y_values), key=y_values.count)\n","    predictions = [most_common for i in range(len(test))]\n","    return predictions\n","\n","predictions = log_reg(X_values, y_values, X_values, 0.1, 100)\n","coefficients = sgd_log(X_values, y_values, learning_rate, epochs)\n","\n","labels = [\"Pregnancies\",\"Glucose\",\"BloodPressure\",\"SkinThickness\",\"Insulin\",\"BMI\",\"DiabetesPedigreeFunction\",\"Age\",\"Outcome\"]\n","for i in range(len(labels)-1):\n","  print(labels[i], \"\\t\", coefficients[i])\n","zeroR_predictions = zeroRC(y_values,y_values)\n","print ('Accuracy:',accuracy(y_values,predictions ))\n","print('ZeroR accuracy:',accuracy(y_values,zeroR_predictions))\n","\n"],"execution_count":23,"outputs":[{"output_type":"stream","name":"stdout","text":["Number of instances: 767\n","Number of features: 9\n","Pregnancies \t -0.6430219595022408\n","Glucose \t 0.21125164196965443\n","BloodPressure \t 0.24539517693029314\n","SkinThickness \t -0.2602827405010931\n","Insulin \t -0.0217362835320609\n","BMI \t 0.09430360859227223\n","DiabetesPedigreeFunction \t -0.004063746372203849\n","Age \t 0.10009764555315388\n","Accuracy: 77.44458930899609\n","ZeroR accuracy: 65.1890482398957\n"]}]},{"cell_type":"markdown","metadata":{"id":"qYMdgnmW30rn"},"source":["Ok. But I've said that using the same data for training and testing is a terrible idea. Below, I'm going to cut the data up into FOUR sets, X_train and y_train for training the data, and X_test and y_test for testing the data. If you've defined X_values and y_values above for this data as you should have, then the following will work. It creates training data that comprises 66% of the total data, and test data that is 33% of the data.\n","\n","The random state is simply a seed value for random selection of isntances. We use a known random seed to give us reproducability of our results.\n","\n","(t) Run the code below, adding in calls to your logistic regression and zeroR functions, and calcualte accuracy. This time, when you use X_train, y_train, X_test and y_test data sets, you'll see that they are DISTINCT. Add in comments comparing the accuracy of this experiment to (s), above.\n"]},{"cell_type":"code","metadata":{"id":"rm8Epsob5-Qe","executionInfo":{"status":"ok","timestamp":1682635602530,"user_tz":240,"elapsed":898,"user":{"displayName":"Manav Bilakhia","userId":"01301071607022221367"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"68f71194-c368-41f2-8abc-acc60e0ee4fa"},"source":["from sklearn.model_selection import train_test_split\n","\n","X_row, X_col = X_values.shape\n","y_row = y_values.shape\n","\n","print('***PRE SELECTION***')\n","print()\n","print('Our original input data (X) is comprised of')\n","print(X_row,'instances and',X_col,'features')\n","print()\n","print('Our original output data (y) is comprised of')\n","print(y_row,'instances')\n","\n","\n","X_train, X_test, y_train, y_test = train_test_split(X_values, y_values, test_size=0.33, random_state=42)\n","\n","X_train_row, X_train_col = X_train.shape\n","y_train_row = y_train.shape\n","\n","X_test_row, X_test_col = X_test.shape\n","y_test_row = y_test.shape\n","\n","print()\n","print('***SLICING the data into TRAIN and TEST data***')\n","print()\n","print(\"TRAINING DATA:\")\n","print('\\tOur training data input (X_train) is comprised of')\n","print('\\t',X_train_row,'instances and',X_train_col,'features')\n","print('\\tOur trainging output (y_train) is comprised of')\n","print('\\t',y_train_row,'instances')\n","\n","print(\"TESTING DATA:\")\n","print('\\tOur testing data input (X_test) is comprised of')\n","print('\\t',X_test_row,'instances and',X_test_col,'features')\n","print('\\tOur testing output (y_test) is comprised of')\n","print('\\t',y_test_row,'instances')\n","print()\n","\n","######\n","#\n","# Call your functions for logistic regression, zeroR, and calculate accuracy\n","# PRINT NICELY\n","#\n","######\n","predictions = log_reg(X_train, y_train, X_test, 0.1, 100)\n","#print('Expected:', y_test)\n","#print('Predicted:', predictions)\n","zeroR_predictions = zeroRC(y_train,X_test)\n","print ('Accuracy:',accuracy(y_test,predictions ))\n","print('ZeroR accuracy:',accuracy(y_test,zeroR_predictions))\n","\n","\n"],"execution_count":24,"outputs":[{"output_type":"stream","name":"stdout","text":["***PRE SELECTION***\n","\n","Our original input data (X) is comprised of\n","767 instances and 8 features\n","\n","Our original output data (y) is comprised of\n","(767,) instances\n","\n","***SLICING the data into TRAIN and TEST data***\n","\n","TRAINING DATA:\n","\tOur training data input (X_train) is comprised of\n","\t 513 instances and 8 features\n","\tOur trainging output (y_train) is comprised of\n","\t (513,) instances\n","TESTING DATA:\n","\tOur testing data input (X_test) is comprised of\n","\t 254 instances and 8 features\n","\tOur testing output (y_test) is comprised of\n","\t (254,) instances\n","\n","Accuracy: 77.95275590551181\n","ZeroR accuracy: 66.53543307086615\n"]}]},{"cell_type":"markdown","metadata":{"id":"l4BxZqYp_P4Y"},"source":["Above is the single, holdout estimation approach. It's a useful first step. But I've told you that we typically prefer k-fold cross-validation. The most frequently used is 10-fold cross-validation. For now, we'll use a value of k=5 (i.e. a five-fold cross validation). We're going to do that TWICE - once will be STRATIFIED, and once without stratification. Make sure you know what that means.  \n","\n","For each of the cells below, I'm creating an instance of the kfold mechanism. We'll start with the version that is NOT STRATIFIED.\n","\n","I'm creating a list to hold my scores from each of my two algorithms. You're going to be running logistic regression and zeroR.\n","\n","I'm splitting my data in X_train, y_train, X_test and y_test data. \n","\n","(u) Then inside the loop, you need to call:\n","- your logistic regression function (using X_train,y_train and y_test), to get a list of predicted y values\n","- your zeroR function (with y_train and X_test), to get a list of predicted y values\n","- your accuracy measure, given y_test and your predicted values for each algorithm\n","- append the accuracy score for logistic regression to lg_scores\n","- append the accuracy score for zeroR to the zr_scores\n","\n","At the completion of the loop, print the following ON ONE LINE per algorithm:\n","- Average accuracy over the 5 runs\n","- MINIMIUM accuracy of the 5 runs\n","- MAXIMUM accuracy of the 5 runs\n","\n","Compare these scores to the single holdout estimation in (t) above, and to the substitution error in (s) above, for both logistic regression and zeroR, and comment on the differences.\n"]},{"cell_type":"code","metadata":{"id":"vP__qDzP7RYf","executionInfo":{"status":"ok","timestamp":1682635606731,"user_tz":240,"elapsed":4206,"user":{"displayName":"Manav Bilakhia","userId":"01301071607022221367"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"16b36f23-3f50-43cc-b67a-d02b72b43dd4"},"source":["from sklearn.model_selection import KFold\n","kf = KFold(n_splits=5,shuffle=True)\n","lg_scores = []\n","zr_scores = []\n","\n","# The loop below will iterate for n_splits times - 5 in this case\n","\n","for train_index, test_index in kf.split(X_values,y_values):\n","    X_train, X_test = X_values[train_index], X_values[test_index]\n","    y_train, y_test = y_values[train_index], y_values[test_index]\n","\n","    # Add in calls to your logistic regression and zeroR functions here\n","    # Then calculate accuracy, and append each score to the appropriate list\n","    # Logistic Regression\n","    y_pred_lg= log_reg(X_train, y_train, X_test, 0.1, 100)\n","    lg_acc = accuracy(y_test, y_pred_lg)\n","    lg_scores.append(lg_acc)\n","\n","    # ZeroR\n","    y_pred_zr = zeroRC(y_train,X_test)\n","    zr_acc = accuracy(y_test, y_pred_zr)\n","    zr_scores.append(zr_acc)\n","\n","# Once the loop is over, calculate and print average score, min and max for each algorithm.\n","lg_avg = sum(lg_scores) / len(lg_scores)\n","lg_min = min(lg_scores)\n","lg_max = max(lg_scores)\n","print(\"Logistic Regression Results:\")\n","print(\"Average accuracy: {:.2f}%\".format(lg_avg))\n","print(\"Minimum accuracy: {:.2f}%\".format(lg_min))\n","print(\"Maximum accuracy: {:.2f}%\".format(lg_max))\n","\n","zr_avg = sum(zr_scores) / len(zr_scores)\n","zr_min = min(zr_scores)\n","zr_max = max(zr_scores)\n","print(\"ZeroR Results:\")\n","print(\"Average accuracy: {:.2f}%\".format(zr_avg))\n","print(\"Minimum accuracy: {:.2f}%\".format(zr_min))\n","print(\"Maximum accuracy: {:.2f}%\".format(zr_max))\n","\n","\n"],"execution_count":25,"outputs":[{"output_type":"stream","name":"stdout","text":["Logistic Regression Results:\n","Average accuracy: 76.14%\n","Minimum accuracy: 71.90%\n","Maximum accuracy: 77.92%\n","ZeroR Results:\n","Average accuracy: 65.18%\n","Minimum accuracy: 62.09%\n","Maximum accuracy: 67.53%\n"]}]},{"cell_type":"markdown","metadata":{"id":"GTWHnSQeBSWe"},"source":["(v) Finally we'll repeat the experiment above, using STRATIFIED K-fold cross-validation. I've created the model. You can fill in the rest, exactly as above. Compare the results of the stratified run to the un-stratified run, above. What's the difference?"]},{"cell_type":"code","metadata":{"id":"KapXvVlRB2ow","executionInfo":{"status":"ok","timestamp":1682635612233,"user_tz":240,"elapsed":5519,"user":{"displayName":"Manav Bilakhia","userId":"01301071607022221367"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"eb2fef07-de79-43e8-ab67-10af8b7540bb"},"source":["from sklearn.model_selection import StratifiedKFold\n","\n","skf = StratifiedKFold(n_splits=5)\n","lg_scores = []\n","zr_scores = []\n","\n","# The loop below will iterate for n_splits times - 5 in this case\n","\n","for train_index, test_index in skf.split(X_values,y_values):\n","    X_train, X_test = X_values[train_index], X_values[test_index]\n","    y_train, y_test = y_values[train_index], y_values[test_index]\n","\n","    # Add in calls to your logistic regression and zeroR functions here\n","    # Then calculate accuracy, and append each score to the appropriate list\n","    # Logistic Regression\n","    y_pred_lg= log_reg(X_train, y_train, X_test, 0.1, 100)\n","    lg_acc = accuracy(y_test, y_pred_lg)\n","    lg_scores.append(lg_acc)\n","\n","    # ZeroR\n","    y_pred_zr = zeroRC(y_train,X_test)\n","    zr_acc = accuracy(y_test, y_pred_zr)\n","    zr_scores.append(zr_acc)\n","\n","# Once the loop is over, calculate and print average score, min and max for each algorithm.\n","lg_avg = sum(lg_scores) / len(lg_scores)\n","lg_min = min(lg_scores)\n","lg_max = max(lg_scores)\n","print(\"Logistic Regression Results:\")\n","print(\"Average accuracy: {:.2f}%\".format(lg_avg))\n","print(\"Minimum accuracy: {:.2f}%\".format(lg_min))\n","print(\"Maximum accuracy: {:.2f}%\".format(lg_max))\n","\n","zr_avg = sum(zr_scores) / len(zr_scores)\n","zr_min = min(zr_scores)\n","zr_max = max(zr_scores)\n","print(\"ZeroR Results:\")\n","print(\"Average accuracy: {:.2f}%\".format(zr_avg))\n","print(\"Minimum accuracy: {:.2f}%\".format(zr_min))\n","print(\"Maximum accuracy: {:.2f}%\".format(zr_max))\n","\n","\n"],"execution_count":26,"outputs":[{"output_type":"stream","name":"stdout","text":["Logistic Regression Results:\n","Average accuracy: 76.66%\n","Minimum accuracy: 73.86%\n","Maximum accuracy: 82.35%\n","ZeroR Results:\n","Average accuracy: 65.19%\n","Minimum accuracy: 64.94%\n","Maximum accuracy: 65.36%\n"]}]}]}