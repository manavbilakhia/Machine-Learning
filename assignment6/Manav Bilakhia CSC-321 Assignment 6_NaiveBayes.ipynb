{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.8"},"colab":{"provenance":[{"file_id":"1143dxm9JxDAhI22UgJ7CsnPLRgrsFyLI","timestamp":1682956749426},{"file_id":"1PAsVQWvWfcol21nUIt6gbqA0TliOdqVV","timestamp":1588784443848}]}},"cells":[{"cell_type":"markdown","metadata":{"id":"42lZmjtvkK4U"},"source":["# CSC-321: Data Mining and Machine Learning\n","# Manav Bilakhia\n","## Assignment 6 Classification with probability\n","\n","### Part 1: Naive Bayes\n","\n","Everything so far has been a linear classifier. Now we'll change gears, and implement some non-linear classifiers. The first, as we saw in class, is Naive Bayes, which makes use of probability to make predictions.\n","\n","We use Bayes' Theorem which allows us to calculate the probability of a piece of data belonging to a given class, given our prior knowledge. Bayes' Theorem is stated as:\n","\n","$$P(class|data) =\\frac{(P(data|class) * P(class))}{P(data)}$$\n","\n","\n","Where P(class|data) is the probability of class given the provided data\n","\n","We're going to break this down into several steps. Again, I've given you an initial contrived data set for you to test your functions. MAKE SURE you verify that you're getting the correct output. Use the slides and the example in class to overstand the overview of how naive bayes works."]},{"cell_type":"markdown","metadata":{"id":"8lk5biXykK4V"},"source":["Conceptually straightforward, there's still a lot to do to get this to work, and I'm going to take the problem bottom up. In order to understand the whole process, I recommend reading EVERYTHING before you start, to make sure you get why we're doing things this way.\n","\n","Ultimately, we're going to do naive bayes prediction on an instance with numeric feature values. That means we need to calculate the gaussian probability density, and as you remember from class, that means we need to gather several statistics on a feature by feature basis. As you also remember, we need to calcualte the probabilities for all the classes. \n","\n","In the early steps, we'll be dealing with TWO input features and TWO class values (0 and 1) BUT DO NOT make any assumptions about the number of input features OR the number of class values.\n","\n","#### (a) Separate by class\n","\n","Just as in the slides, we need to calculate the probability of data by the class they belong to. We're going to do this in stages. Read all of the steps before coding. First, we need to separate our data by the class values. \n","\n","Our dataset contains TWO input features, and one class value. We'll assume (as we often do) that the LAST value of each instance is the class value.\n","\n","Taking our dataset, create two variables, called X_values (containing a list of lists of input values) and y_values (a list of the class values). Pass these to a function that will create and return a dictionary, where the key is a class value, and the corresponding dictionary value is a list of all input instances with that class value. "]},{"cell_type":"code","metadata":{"id":"mN7qTGH3kK4W","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1683144251422,"user_tz":240,"elapsed":162,"user":{"displayName":"Manav Bilakhia","userId":"01301071607022221367"}},"outputId":"e6ec7ba6-0539-4aaa-dab1-877d4bf7601d"},"source":["# Contrived data set\n","\n","dataset = [[3.393533211,2.331273381,0],\n","    [3.110073483,1.781539638,0],\n","    [1.343808831,3.368360954,0],\n","    [3.582294042,4.67917911,0],\n","    [2.280362439,2.866990263,0],\n","    [7.423436942,4.696522875,1],\n","    [5.745051997,3.533989803,1],\n","    [9.172168622,2.511101045,1],\n","    [7.792783481,3.424088941,1],\n","    [7.939820817,0.791637231,1]]\n","\n","def separate_by_class(X_values, y_values):\n","    separated = {}\n","    for i in range(len(X_values)):\n","        instance = X_values[i]\n","        class_value = y_values[i]\n","        if class_value not in separated:\n","            separated[class_value] = []\n","        separated[class_value].append(instance)\n","    return separated\n","\n","X_values = [row[:-1] for row in dataset]\n","y_values = [row[-1] for row in dataset]\n","\n","separated = separate_by_class(X_values, y_values)\n","print(separated)\n","\n","\n"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["{0: [[3.393533211, 2.331273381], [3.110073483, 1.781539638], [1.343808831, 3.368360954], [3.582294042, 4.67917911], [2.280362439, 2.866990263]], 1: [[7.423436942, 4.696522875], [5.745051997, 3.533989803], [9.172168622, 2.511101045], [7.792783481, 3.424088941], [7.939820817, 0.791637231]]}\n"]}]},{"cell_type":"markdown","metadata":{"id":"zElUAIq3kK4a"},"source":["#### (b) Summarize the data\n","\n","We are going to need two statistics from the data, the mean and the standard deviation. You should have the base of these functions in a previous assignment, remembering that the standard deviation is simply the square root of the variance. \n","\n","For this assignment, we need to compute the **average variance**, not the total variance as we did with SLR. Also, we're using a **SAMPLE** and not a **POPULATION**. You should know what that means with respect to what we divide by. Refer to the slides on SLR if you don't remember. \n","\n","In the contrived data, we have 3 features - 2 input features and an output feature, y.\n","\n","We need the mean and standard deviation for each of our input features. Create a function that summarizes a given set of X_values instances, by calculating the mean and standard deviation on that data. We'll collect this information into a tuple, one per column, comprising the mean, the standard deviation and the number of elements in each column). Return a list of these tuples.\n","\n","REMEMBER: Do NOT do anything that relies on the fact that are only 2 input features in this data. \n","\n","More generally, the output should be:\n","\n","[(feature1_mean,feature1_std,feature1_count), (feature2_mean,feature2_std,feature2_count),....,(featureN_mean,featureN_std,featureN_count)]\n"]},{"cell_type":"code","metadata":{"id":"wgpHs0sGkK4a","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1683144251643,"user_tz":240,"elapsed":5,"user":{"displayName":"Manav Bilakhia","userId":"01301071607022221367"}},"outputId":"564511ed-fc46-4f11-b6d3-9c08bdd76784"},"source":["# implement your function here, and copy across any functions you need to help you\n","import math\n","\n","def summarize(dataset):\n","    \"\"\"Calculates the mean, standard deviation, and count for each feature in the input dataset.\"\"\"\n","    \n","    # Initialize an empty list to hold the summary information for each feature\n","    summaries = []\n","    # Iterate over each feature in the dataset\n","    for feature in zip(*dataset):\n","        # Calculate the mean of the feature\n","        mean = sum(feature) / len(feature)\n","        \n","        # Calculate the variance of the feature (using sample variance)\n","        variance = sum([(x - mean) ** 2 for x in feature]) / (len(feature) - 1)\n","        \n","        # Calculate the standard deviation of the feature\n","        std_dev = math.sqrt(variance)\n","        \n","        # Add the summary information to the list of summaries\n","        summaries.append((mean, std_dev, len(feature)))\n","    \n","    return summaries\n","\n","print (summarize(dataset))\n","\n","\n"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[(5.178333386499999, 2.7665845055177263, 10), (2.9984683241, 1.218556343617447, 10), (0.5, 0.5270462766947299, 10)]\n"]}]},{"cell_type":"markdown","metadata":{"id":"sFmFpy8ukK4d"},"source":["#### (c) Summarize data by class\n","\n","We now need to combine the functions from (a) and (b) above. Create a  function, that takes X_values and y_values. Your function should split the data by class using your function from (a). \n","\n","It then calculates statistics for all instances of the data for each class using (b), getting a summary of the feature values for each feature, for each class.\n","\n","The results - the list of tuples of statistics, one per column - should then be stored in a dictionary by their class value. summarizeByClass should return such a dictionary. I include my output on the included dataset for verification. "]},{"cell_type":"code","metadata":{"id":"hZgK1NxYkK4e","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1683144251643,"user_tz":240,"elapsed":4,"user":{"displayName":"Manav Bilakhia","userId":"01301071607022221367"}},"outputId":"b4c4da80-b9f0-4acf-81f1-e9c99cb28afb"},"source":["\n","# implement your function here\n","def summarizeByClass(X_values, y_values):\n","    separated = separate_by_class(X_values, y_values)\n","    summaries = {}\n","    for class_value, instances in separated.items():\n","        summaries[class_value] = summarize(instances)\n","    return summaries\n","summary = summarizeByClass(X_values, y_values)\n","print(summary)\n","\n","\n","# The output dictionary for the contrived data should look like:\n","# {0: [(2.7420144012, 0.9265683289298018, 5), (3.0054686692, 1.1073295894898725, 5)], 1: [(7.6146523718, 1.2344321550313704, 5), (2.9914679790000003, 1.4541931384601618, 5)]}\n"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["{0: [(2.7420144012, 0.9265683289298018, 5), (3.0054686692, 1.1073295894898725, 5)], 1: [(7.6146523718, 1.2344321550313704, 5), (2.9914679790000003, 1.4541931384601618, 5)]}\n"]}]},{"cell_type":"markdown","metadata":{"id":"tcAyj5fgkK4i"},"source":["#### (d) Guassiaun Probability Density\n","\n","We're working with features that contain numerical rather than nominal data here, so we need to implement the gaussian probability density function (PDF) we talked about in class, so we can attach probabilities to real values. A gaussian distribution can be summarized from two values - mean and standard deviation. The gaussian PDF is calculated as follows:\n","\n","\n","$$probability(x) = \\frac{1}{\\sqrt{ 2 * \\pi } * \\sigma}*e^-(\\frac{(x-mean(x))^2}{2 * \\sigma^2})$$\n","\n","Where:\n","- sigma is the standard deviation\n","- e is Euler's number (math.exp(x)) \n","\n","Hopefully, you can see why we're going to need the mean and the std_dev from function (c). I find it incredibly helpful to calculate the above equation in stages - i.e. calculate the first part of the equation and the exponent separately, then combine them at the end.\n","\n","Create a function that:\n","- takes a value (x)\n","- takes a mean\n","- takes a standard deviation\n","\n","and returns the probability of seeing that value, using the formula above."]},{"cell_type":"code","metadata":{"id":"fto11aKikK4j"},"source":["# Implement your function here\n","import math\n","\n","def gaussian_probability_density(x, mean, std_dev):\n","    first_part = 1 / (math.sqrt(2 * math.pi) * std_dev)\n","    exponent = math.exp(-((x - mean) ** 2) / (2 * std_dev ** 2))\n","    return first_part * exponent\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LcFcLnQ0kK4m"},"source":["#### (e) Class Probabilities\n","\n","We can now use probabilites from our training data to calculate the class probabilities for any instance of new data, by creating a function called calculateProbabilities. \n","\n","Probabilites have to be calculated separately for each possible class value; for each class value we have to calculate the likelihood that the new instance belongs to that class value. This is exactly what I work through on the slides, so refer to those if it's helpful. The probability that a piece of data belongs to a class value is calculated by:\n","\n","$$P(class|data) =(P(data|class) * P(class))$$\n","\n","\n","The divison from Bayes' Theorem has been removed, because we're just trying to *maximize* the result of the formula above. The largest value we get for a class value determines which class value we assign. In the case where we have TWO input features in our data (X1 and X2), the probablility that an instance belongs to class value 0 is calculated by:\n","\n","$$P(class=0|X_{1},X_{2}) = P(X_{1}|class=0) * P(X_{2}|class=0) * P(class=0)$$\n","\n","We have to repeat this for each class value, and then choose the class value with the highest score. We should not assume a fixed number of input features, X, the above was just an illustration. More generally it is:\n","\n","$$P(class=0|X_{1},X_{2}) = P(X_{1}|class=0) * P(X_{2}|class=0) *  ... * P(X_{N}|class=0) * P(class=0)$$\n","\n","We'll start by creating a function that will return the probabilities of predicting each class value for a given instance. \n","\n","This function will take a dictionary of summaries (as returned by (c), above) and an instance, and will generate a dictionary of probabilites, with one entry per class. The steps are as follows:\n","\n","- We need to calculate the total number of training instances, by counting the counts stored in the summary statistics. So if there are 9 instances with one label, and 5 with another (as in the weather data) then we need to know there are 14 instances total. Thankfully, in the data from function (c), we know where to find those counts for each class. \n","\n","- This will help us calculate the prior probability, P(class), as the ratio of rows with a given class divided by all rows in the training data\n","\n","- Next probabilities are calculated for each feature value in the instance to be classified. Using the function from (d), we can generate a probability of associating a feature value with a class value. Those probabilites are multiplied together as they are accumulated with the formula given above. \n","\n","- The process is repeated for each class in the data\n","\n","- Return the dictionary of probabilities for each class for the new instance\n","\n","Some things that might help with implementation. \n","\n","- Dictionaries are your friend here\n","- List comprehensions are also your friend\n","- The data returned by (c) above is already divided by class. You can:\n","    - discover the prior probability from this data (how many instances for this class, divided by the total instances)\n","    - iterate over the tuples, which give you the information (mean, std_dev, count) on a per column basis\n","    - calculate probability given the attribute value corresponding to that column using your function from (d)\n","\n","Try this out on the contrived data. You should be able to calculate all the relevant scores by hand to determine if your code is accurate.\n","\n","NOTE: If you want to output ACTUAL probabilities by class, we divide each score in the dictionary for an instance, by the sum of the values. You don't need to do this, as I've included code to do it for you.\n"]},{"cell_type":"code","metadata":{"id":"bhH_yTcTkK4n","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1683144251643,"user_tz":240,"elapsed":3,"user":{"displayName":"Manav Bilakhia","userId":"01301071607022221367"}},"outputId":"f48b171f-9a6c-43c2-b86a-4ac4908f398a"},"source":["\n","# Implement your function here\n","\n","import math\n","\n","def calculateProbabilities(summaries, input_vector):\n","    \"\"\"\n","    Calculates the probability of each class given the input vector.\n","\n","    Arguments:\n","    summaries -- a dictionary of summary statistics for each class\n","    input_vector -- a list of values representing a data instance to be classified\n","\n","    Returns:\n","    A dictionary of probabilities, one for each class value\n","    \"\"\"\n","    # count the total number of instances\n","    total_instances = sum(summary[0][2] for summary in summaries.values())\n","\n","    # initialize a dictionary to store probabilities for each class\n","    probabilities = {}\n","\n","    # calculate probabilities for each class value\n","    for class_value, class_summary in summaries.items():\n","        # calculate the prior probability of this class\n","        class_probability = class_summary[0][2] / total_instances\n","\n","        # calculate likelihood probability for each attribute value given the class\n","        likelihood = 1.0\n","        for i in range(len(class_summary)):\n","            mean, stdev, count = class_summary[i]\n","            x = input_vector[i]\n","            likelihood *= gaussian_probability_density(x, mean, stdev)\n","\n","        # calculate the posterior probability for this class\n","        probabilities[class_value] = class_probability * likelihood\n","\n","    # normalize probabilities\n","    total_prob = sum(probabilities.values())\n","    for class_value in probabilities:\n","        probabilities[class_value] /= total_prob\n","\n","    return probabilities\n","\n","# Test it out here\n","# Get a bunch of summaries from your function from (c)\n","# Pass those summaries (a dictionary) to your function from (e)\n","# in order to predict the class of the test instance, below.\n","# PRINT OUT the dictionary returned by calculateProbabilities\n","# And compare to my score, below\n","\n","test_instance = X_values[0]\n","summaries = summarizeByClass(X_values, y_values)\n","probabilities = calculateProbabilities(summaries, test_instance)\n","print(probabilities)\n","\n","\n","\n","# I think if everything works, the returned value from (e) should be:\n","# {0: 0.05032427673372075, 1: 0.00011557718379945765}\n","# which according to the percentage calculation given should be:\n","# 99.77% in favour of class 0 \n","# Code to compute this is given below, assuming you store the dictionary returned\n","# from calculateProbabilities in a variable called probabilities\n","\n","print()\n","sumProbs = sum([value for _,value in probabilities.items()])\n","for key,value in probabilities.items():\n","    normProb = value / sumProbs * 100\n","    print('The probability of the instance belonging to class {} is {:.2f}'.format(key,normProb))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["{0: 0.9977086138276996, 1: 0.0022913861723003934}\n","\n","The probability of the instance belonging to class 0 is 99.77\n","The probability of the instance belonging to class 1 is 0.23\n"]}]},{"cell_type":"markdown","metadata":{"id":"ets2xIjTkK4q"},"source":["#### (f) Tying it all together\n","\n","You need to create a predict function. This function works very much as the example above, in that it takes a dictionary of summaries and a single row, and uses calculateProbabilites to get the resulting dictionary of probabilities. From this dictionary, find the largest value and corresponding class. Return this class. \n","\n","You also need a naiveBayes function, that takes a training set (X,train, y_train) and a test set (X_test). It needs to generate summary statistics from the training set (using (c), above), then make predictions for each instance in the test set, by calling your predict function for each instance, using the summaries generated. Append these predictions to a list you return.\n","\n","Print out the list of actual values and predicted values, for now using the same data for training and testing."]},{"cell_type":"code","metadata":{"id":"-T9_e7XokK4q","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1683144251796,"user_tz":240,"elapsed":155,"user":{"displayName":"Manav Bilakhia","userId":"01301071607022221367"}},"outputId":"dbb16feb-d896-40e8-b56b-af4392becf6e"},"source":["\n","# Implement predict(summaries,instance) here\n","def predict(summaries, instance):\n","    probabilities = calculateProbabilities(summaries, instance)\n","    best_label, best_prob = None, -1\n","    for class_value, probability in probabilities.items():\n","        if best_label is None or probability > best_prob:\n","            best_prob = probability\n","            best_label = class_value\n","    return best_label\n","\n","# Implement naive_bayes(X_train,y_train,X_test) here\t\n","\n","def naive_bayes(X_train, y_train, X_test):\n","    summaries = summarizeByClass(X_train, y_train)\n","    predictions = []\n","    for instance in X_test:\n","        prediction = predict(summaries, instance)\n","        predictions.append(prediction)\n","    return predictions\n","\n","\n","predictions = naive_bayes(X_values, y_values, X_values)\n","\n","# print actual and predicted values\n","print(\"Actual values:\", y_values)\n","print(\"Predicted values:\", predictions)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Actual values: [0, 0, 0, 0, 0, 1, 1, 1, 1, 1]\n","Predicted values: [0, 0, 0, 0, 0, 1, 1, 1, 1, 1]\n"]}]},{"cell_type":"markdown","metadata":{"id":"ZXjC4ZLYkK4u"},"source":["### Part 2: Applying to real data\n","\n","You've seen bits of the iris dataset in class. It's one of the most well known data sets in machine learning and data mining. So you might as well have a go at it! You can find out more about it here: https://en.wikipedia.org/wiki/Iris_flower_data_set\n","\n","I don't need to use pandas to load the data, as the iris data set is included with scikit learn. Below, I'm going to load the data for you. You have to:\n","\n","- call your naive bayes algorithm, using a Stratified 5-fold cross-validation\n","- compare this to some reasonable baseline, using your code\n","- give me a short write up of the results\n","\n","This will require checking back at how I did things last week, with respect to doing a Stratified 5-fold cross-validation."]},{"cell_type":"code","metadata":{"id":"4mkDk8XdvCFI","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1683144253157,"user_tz":240,"elapsed":1362,"user":{"displayName":"Manav Bilakhia","userId":"01301071607022221367"}},"outputId":"44fa56e1-3f6e-4724-8c2e-9f53194a269f"},"source":["# Do part 2 here\n","\n","from sklearn import datasets\n","from sklearn.model_selection import StratifiedKFold\n","from sklearn.metrics import accuracy_score\n","from collections import defaultdict\n","import numpy as np\n","\n","\n","iris = datasets.load_iris()\n","X_values = iris.data\n","y_values = iris.target\n","\n","rows,cols = X_values.shape\n","print(\"This is the IRIS training data set. It has\", rows, \"instances, and it has\", cols, \"features.\\n\")\n","\n","# The below is only to show you what the class values are\n","# You do NOT need to use class_values anywhere\n","\n","class_values = set(y_values)\n","print(\"The IRIS test data class has the folowing values:\", class_values, \"\\n\")\n","\n","\n","skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n","\n","naive_bayes_acc = []\n","baseline_acc = []\n","\n","for train_index, test_index in skf.split(X_values, y_values):\n","    X_train, X_test = X_values[train_index], X_values[test_index]\n","    y_train, y_test = y_values[train_index], y_values[test_index]\n","\n","    # Naive Bayes classification\n","    naive_bayes_pred = naive_bayes(X_train, y_train, X_test)\n","    naive_bayes_acc.append(accuracy_score(y_test, naive_bayes_pred))\n","\n","    # Baseline classification (always predicting the most frequent class)\n","    baseline_pred = [np.bincount(y_train).argmax()] * len(y_test)\n","    baseline_acc.append(accuracy_score(y_test, baseline_pred))\n","\n","print(\"Naive Bayes accuracy:\", np.mean(naive_bayes_acc))\n","print(\"Baseline accuracy:\", np.mean(baseline_acc))\n"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["This is the IRIS training data set. It has 150 instances, and it has 4 features.\n","\n","The IRIS test data class has the folowing values: {0, 1, 2} \n","\n","Naive Bayes accuracy: 0.9466666666666667\n","Baseline accuracy: 0.3333333333333333\n"]}]},{"cell_type":"markdown","metadata":{"id":"UHEenm4FFUwC"},"source":["Write up here"]},{"cell_type":"markdown","metadata":{"id":"Whpk2fJ7D54w"},"source":["## Part 3: Introduction to scikit-learn\n","\n","One of the most popular open-source python machine learning libraries is scikit-learn.\n","\n","As we go through this class I'll introduce you to some of the functionality. Below I want you to use Gaussian Naive Bayes.\n","\n","You can find out more in general at: https://scikit-learn.org/stable/index.html\n","\n","This time, I'm only doing the bare minimum. I'll load the relevant models from scikit-learn, but it's up to you to train and test them, and report the scores appropriately. Your scores should be the same as your code, above.\n","\n","You need to:\n","- Use [gaussian naive bayes](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html)\n","- Use a reasonable baseline\n","- Use a reasonable measure of performance\n","- Perform a stratified 5 fold cross validation\n","- Collect the results\n","- Print the mean, the min and the max of each cross-validation experiment\n"]},{"cell_type":"code","metadata":{"id":"8HZZ-JpqkK4u","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1683144253158,"user_tz":240,"elapsed":4,"user":{"displayName":"Manav Bilakhia","userId":"01301071607022221367"}},"outputId":"0da00d60-4f68-40cf-826c-b2034293e8c8"},"source":["\n","#Do part 3 here\n","from sklearn.naive_bayes import GaussianNB\n","from sklearn.model_selection import cross_val_score, StratifiedKFold\n","from sklearn.dummy import DummyClassifier\n","\n","#Creating the Gaussian Naive Bayes model\n","gnb = GaussianNB()\n","\n","#Creating the dummy classifier for baseline comparison\n","dummy_clf = DummyClassifier(strategy=\"most_frequent\")\n","\n","#Setting up the cross-validation\n","cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n","\n","#Calculating the cross-validation scores for Gaussian Naive Bayes\n","gnb_scores = cross_val_score(gnb, X_values, y_values, cv=cv)\n","\n","#Calculating the cross-validation scores for the dummy classifier\n","dummy_scores = cross_val_score(dummy_clf, X_values, y_values, cv=cv)\n","\n","#Printing the mean, min, and max scores for each model\n","print(\"Gaussian Naive Bayes scores: Mean: {:.3f}, Min: {:.3f}, Max: {:.3f}\".format(gnb_scores.mean(), gnb_scores.min(), gnb_scores.max()))\n","print(\"Dummy Classifier scores: Mean: {:.3f}, Min: {:.3f}, Max: {:.3f}\".format(dummy_scores.mean(), dummy_scores.min(), dummy_scores.max()))\n","\n","\n"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Gaussian Naive Bayes scores: Mean: 0.947, Min: 0.900, Max: 1.000\n","Dummy Classifier scores: Mean: 0.333, Min: 0.333, Max: 0.333\n"]}]}]}